{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0666fbb",
   "metadata": {},
   "source": [
    "# Lab 1 — Tiny VLM Adversarial Cost Challenge\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to the ML Security Lab! In this lab, you'll implement adversarial attacks against a Vision-Language Model (VLM) using the custom dataset and a frozen TinyCLIP scorer.\n",
    "\n",
    "### Objective\n",
    "Your task is to build an attack function that can manipulate either:\n",
    "- **Caption tokens** (text modifications)\n",
    "- **Image pixels** (visual modifications)\n",
    "- **Both** (multimodal attack)\n",
    "\n",
    "The goal is to flip the model's decision (match → no-match or vice versa) while minimizing the attack cost.\n",
    "\n",
    "### Constraints\n",
    "- **T_MAX = 10**: Maximum token edits per sample\n",
    "- **P_MAX = 100**: Maximum pixel edits per sample  \n",
    "- **Q_MAX = 100**: Maximum queries per sample\n",
    "- **Evaluation**: Public leaderboard (1,000 val pairs) + Private leaderboard (1,000 test pairs)\n",
    "\n",
    "### Scoring\n",
    "Your attack will be evaluated based on:\n",
    "1. **Success Rate**: Percentage of samples where you successfully flip the decision\n",
    "2. **Cost Efficiency**: Lower total cost (token edits + pixel edits + queries) is better\n",
    "3. **Attack Budget**: Must stay within the specified limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5244d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required data is already present.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Check if 'images' directory or 'val_pairs.json' file is missing\n",
    "if not os.path.exists('images') or not os.path.exists('val_pairs.json'):\n",
    "    print(\"Required data not found. Extracting 'data.zip'...\")\n",
    "    \n",
    "    # Check if 'data.zip' exists\n",
    "    if os.path.exists('data.zip'):\n",
    "        with zipfile.ZipFile('data.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall()  # Extract all files in the current directory\n",
    "        print(\"Extraction complete!\")\n",
    "    else:\n",
    "        print(\"Error: 'data.zip' not found. Please ensure the file is in the current directory.\")\n",
    "else:\n",
    "    print(\"All required data is already present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab690789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/adverserial-cybersecurity/.venv/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages imported successfully!\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import open_clip\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"All packages imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436ae18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data...\n",
      "Loaded 1000 validation pairs\n",
      "\n",
      "Sample validation pairs:\n",
      "  Image ID: 38137, Path: images/val/000000119402.jpg\n",
      "  Caption: A gold bus traveling on a single lane road..., Match: True\n",
      "\n",
      "  Image ID: 15194, Path: images/val/000000404780.jpg\n",
      "  Caption: Two women in the snow on skis in front of a large ..., Match: False\n",
      "\n",
      "  Image ID: 19082, Path: images/val/000000148898.jpg\n",
      "  Caption: A man jumping a brown horse over an obstacle...., Match: False\n",
      "\n",
      "Data distribution:\n",
      "  Match (True): 500\n",
      "  No-match (False): 500\n",
      "  Balance: 50.00% positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (']\n",
      "Bad pipe message: %s [b'TML, like Gecko) Chrome/141.0.0.0 Safari/537.36\\r\\nAccept-Encoding: gzip, de']\n",
      "Bad pipe message: %s [b'ate, br, zstd\\r\\nAccept-Language: en-US,en;q=0.9,da;q=0.8,da-DK;q=0.7,de;q=0.6\\r\\nX-Request-ID: a2acf30f9']\n",
      "Bad pipe message: %s [b'c18a1ff381a4e46b476a3\\r\\nX-Real-IP: 130.225.91.23']\n",
      "Bad pipe message: %s [b'\\nX-Forwarded-Port: 443\\r\\nX-Forwarded-Scheme: https\\r', b'-Original']\n",
      "Bad pipe message: %s [b'RI: /\\r\\nX-Scheme: https\\r\\nsec-fetch-site: none', b'sec-fetch-mo']\n",
      "Bad pipe message: %s [b': cors\\r\\nsec-fetch-dest: empty\\r\\nsec-fetch-storage-access: active\\r\\npriority: u=1, i\\r\\nX-Forwarded-Prot', b' https\\r\\nX-Forwarded-Host: 63vxb49s-40725.euw.devtunnels.ms\\r\\nX-Forwarded-For: 130.225.91.233\\r\\nProxy-Connection:']\n",
      "Bad pipe message: %s [b'svg+xml,image/*,*/*;q=0.8\\r\\nHost: localhost:40725\\r\\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x', b') AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0']\n",
      "Bad pipe message: %s [b'.0 Safari/537.36\\r\\nAccept-Encoding: gzip, defl']\n",
      "Bad pipe message: %s [b'e, br, zstd\\r\\nAccept-Language: en-US,en;q=0.9,da;q=0.8,da-DK;q=0.7,de;q=0.6\\r\\nReferer: https://63v']\n",
      "Bad pipe message: %s [b'49s-40725.euw.devtunnels.ms/\\r\\nX-Request-ID: 757a9f6bf7bcc63db03f05d66e1785a3\\r\\nX-Real-IP: 130.225.91.233\\r\\nX-Forwarded-Po']\n",
      "Bad pipe message: %s [b': 443\\r\\nX-Forwarded-Scheme: https\\r\\nX-Original-URI: /favicon.ico\\r\\nX-Scheme: https\\r\\nsec-ch-ua-platform: \"Windows\"\\r\\ns']\n",
      "Bad pipe message: %s [b'-ch-ua: \"Google Chrome\";v=\"141\", \"Not?A_Brand\";v=\"8\", \"Chromium\";v=\"141\"\\r\\nsec-ch-ua-mobile: ?0\\r\\nsec-', b'tch-site: same-origin\\r\\nsec-fetch-mode: no-cors\\r\\nsec-fetch-dest: image\\r\\npriority: u=1, i\\r\\nX-Forwarded-', b'oto: https\\r\\nX-Forwarded-Host: 63vxb49s-40725.euw.devtunnels.ms\\r\\nX-Forwarded-For']\n",
      "Bad pipe message: %s [b'0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\\r\\nHost: localhost:40725\\r\\nUs', b'-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.']\n",
      "Bad pipe message: %s [b'0.0 Safari/537.36\\r\\nAccept-Encoding: gzip, defla']\n",
      "Bad pipe message: %s [b', br, zstd\\r\\nAccept-Language: en-US,en;q=0.9,da;q=0.8,da-DK;q=0.7,de;q=0.6\\r\\nCache-Control: max-age=0\\r\\nReferer: https', b'/63vxb49s-40725.euw.devtunnels.ms/\\r\\nX-Request-ID: 82f1889', b'86cdb416703c0ec5c7ac1ba\\r\\nX-Real-IP: 130.225.91.233\\r\\nX-Forwarded-Port: 443\\r\\nX-Forwarded-Scheme: ht', b's\\r\\nX-Original-URI: /\\r\\nX-Scheme: https\\r\\nsec-ch-ua: \"Google Chrome\";v=\"141\", \"Not?A_Brand\";v=\"8\", \"Chromium\";v=\"141\"\\r']\n",
      "Bad pipe message: %s [b'ec-ch-ua-', b'bile: ?0\\r\\nsec-ch-ua-platform: \"Windows\"\\r\\nsec-fetch-site: same-origin\\r\\nsec-fetch-mode: navigate\\r\\nsec-fetch-us', b': ?1\\r\\nsec-fetch-dest: document\\r\\npriority: u=0, i\\r\\nX-Forwarded-Proto: https\\r\\nX-Forwarded-Host: 63vxb4']\n",
      "Bad pipe message: %s [b'0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\\r\\nHost: localhost:40725\\r\\nUs', b'-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.']\n",
      "Bad pipe message: %s [b'0.0 Safari/537.36\\r\\nAccept-Encoding: gzip, defla']\n",
      "Bad pipe message: %s [b', br, zstd\\r\\nAccept-Language: en-US,en;q=0.9,da;q=0.8,da-DK;q=0.7,de;q=0.6\\r\\nCache-Control: max-age=0\\r\\nReferer: https', b'/63vxb49s-40725.euw.devtunnels.ms/\\r\\nX-Request-ID: 97e976a', b'59b0ebfad45397a41f3d39e\\r\\nX-Real-IP: 130.225.91.233\\r\\nX-Forwarded-Port: 443\\r\\nX-Forwarded-Scheme: https']\n",
      "Bad pipe message: %s [b'X-Original-U']\n",
      "Bad pipe message: %s [b': /\\r\\nX-Scheme: https\\r\\nsec-ch-ua: \"Google Chrome\";v=\"141\", \"Not?A_Brand\";v=\"8\", \"C', b'omium\";v=\"141\"\\r\\nsec-ch-ua-mobile: ?0\\r\\nsec-ch-ua-platform: \"Windows\"\\r\\nsec-fetch-site: same-origin\\r\\nsec-f']\n",
      "Bad pipe message: %s [b'ch-mode: navigate\\r\\nsec-fetch-user: ?1\\r\\nsec-fetch-dest: document\\r\\npriority: u=0, i\\r\\nX-Forwarded-Proto']\n",
      "Bad pipe message: %s [b'https\\r\\nX-Forwarded-Host: 63vxb49s-40725.euw.devtunnels.ms']\n",
      "Bad pipe message: %s [b'X-Forwarded-']\n"
     ]
    }
   ],
   "source": [
    "# Data Loading\n",
    "print(\"Loading validation data...\")\n",
    "\n",
    "# Load validation pairs from JSON\n",
    "with open('val_pairs.json', 'r') as f:\n",
    "    val_pairs = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(val_pairs)} validation pairs\")\n",
    "\n",
    "# Helper function to load images\n",
    "def load_image_from_pair(pair: dict) -> Image.Image:\n",
    "    \"\"\"Load image from the pair dictionary using image_path\"\"\"\n",
    "    return Image.open(pair['image_path']).convert('RGB')\n",
    "\n",
    "# Sample a few pairs to verify data loading\n",
    "print(\"\\nSample validation pairs:\")\n",
    "for i in range(3):\n",
    "    pair = val_pairs[i]\n",
    "    print(f\"  Image ID: {pair['image_id']}, Path: {pair['image_path']}\")\n",
    "    print(f\"  Caption: {pair['caption'][:50]}..., Match: {pair['is_match']}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Data distribution:\")\n",
    "labels = [pair['is_match'] for pair in val_pairs]\n",
    "print(f\"  Match (True): {sum(labels)}\")\n",
    "print(f\"  No-match (False): {len(labels) - sum(labels)}\")\n",
    "print(f\"  Balance: {sum(labels)/len(labels):.2%} positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f88fe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model...\n",
      "Failed to load TinyCLIP: Failed initial config/weights load from HF Hub microsoft/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M: Failed to download file (open_clip_config.json) for microsoft/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M. Last error: 404 Client Error. (Request ID: Root=1-68e83bb3-161e0a867af9421049bac461;06432bb6-83af-48c4-bd5b-c1bd96b9066e)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/microsoft/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M/resolve/main/open_clip_config.json.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "Falling back to OpenCLIP ViT-B/32...\n",
      "Successfully loaded OpenCLIP ViT-B/32\n",
      "Model loaded on: cuda\n"
     ]
    }
   ],
   "source": [
    "# TinyCLIP Scorer Implementation\n",
    "print(\"Loading CLIP model...\")\n",
    "\n",
    "# Try to load TinyCLIP, fallback to OpenCLIP ViT-B/32 if failed\n",
    "try:\n",
    "    # Attempt to load TinyCLIP from HuggingFace hub\n",
    "    model, preprocess, tokenizer = open_clip.create_model_and_transforms(\n",
    "        \"hf-hub:microsoft/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M\"\n",
    "    )\n",
    "    print(\"Successfully loaded TinyCLIP model\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load TinyCLIP: {e}\")\n",
    "    print(\"Falling back to OpenCLIP ViT-B/32...\")\n",
    "\n",
    "    model, preprocess, tokenizer = open_clip.create_model_and_transforms(\n",
    "        \"ViT-B-32\", \n",
    "        pretrained=\"laion2b_s34b_b79k\"\n",
    "    )\n",
    "    print(\"Successfully loaded OpenCLIP ViT-B/32\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3728b433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing CLIP embedding function...\n",
      "Sample similarity score: 0.3695 (Expected match: True)\n",
      "\n",
      "Testing on more samples:\n",
      "Sample 1: similarity=0.3526, match=True\n",
      "Sample 2: similarity=0.0343, match=False\n",
      "Sample 3: similarity=0.0774, match=False\n"
     ]
    }
   ],
   "source": [
    "def clip_embed(image: Image.Image, caption: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between image and text embeddings.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        caption: Text string\n",
    "    \n",
    "    Returns:\n",
    "        Cosine similarity score between normalized embeddings\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Preprocess image\n",
    "        image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Tokenize text properly using open_clip tokenizer\n",
    "        text_tokens = open_clip.tokenize([caption]).to(device)\n",
    "        \n",
    "        # Get embeddings\n",
    "        image_features = model.encode_image(image_tensor)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = (image_features @ text_features.T).item()\n",
    "        \n",
    "    return similarity\n",
    "\n",
    "# Test the embedding function\n",
    "print(\"\\nTesting CLIP embedding function...\")\n",
    "test_pair = val_pairs[0]\n",
    "test_image = load_image_from_pair(test_pair)\n",
    "test_similarity = clip_embed(test_image, test_pair['caption'])\n",
    "print(f\"Sample similarity score: {test_similarity:.4f} (Expected match: {test_pair['is_match']})\")\n",
    "\n",
    "# Test on a few more samples\n",
    "print(\"\\nTesting on more samples:\")\n",
    "for i in range(3):\n",
    "    pair = val_pairs[i]\n",
    "    image = load_image_from_pair(pair)\n",
    "    similarity = clip_embed(image, pair['caption'])\n",
    "    print(f\"Sample {i+1}: similarity={similarity:.4f}, match={pair['is_match']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d850a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating scorer with logistic regression...\n",
      "Using 200 samples for calibration\n",
      "Computing similarities for calibration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibration: 100%|██████████| 200/200 [00:05<00:00, 37.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration complete!\n",
      "   Alpha (slope): 6.6231\n",
      "   Beta (intercept): -1.1130\n",
      "\n",
      "Calibration test:\n",
      "  Sim: 0.3561 → Prob: 0.7765, True: 1\n",
      "  Sim: 0.0343 → Prob: 0.2920, True: 0\n",
      "  Sim: 0.0802 → Prob: 0.3585, True: 0\n",
      "  Sim: 0.1899 → Prob: 0.5362, True: 0\n",
      "  Sim: 0.3586 → Prob: 0.7794, True: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calibration: Fit logistic regression to get alpha, beta parameters\n",
    "print(\"Calibrating scorer with logistic regression...\")\n",
    "\n",
    "# Use first 200 samples for calibration\n",
    "tune_slice = val_pairs[:200]\n",
    "print(f\"Using {len(tune_slice)} samples for calibration\")\n",
    "\n",
    "# Compute similarities for calibration\n",
    "similarities = []\n",
    "ground_truths = []\n",
    "\n",
    "print(\"Computing similarities for calibration...\")\n",
    "for pair in tqdm(tune_slice, desc=\"Calibration\"):\n",
    "    image = load_image_from_pair(pair)\n",
    "    similarity = clip_embed(image, pair['caption'])\n",
    "    similarities.append(similarity)\n",
    "    ground_truths.append(int(pair['is_match']))\n",
    "\n",
    "similarities = np.array(similarities).reshape(-1, 1)\n",
    "ground_truths = np.array(ground_truths)\n",
    "\n",
    "# Fit logistic regression: sigmoid(alpha * cosine + beta)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(similarities, ground_truths)\n",
    "\n",
    "# Extract alpha and beta\n",
    "alpha = lr.coef_[0][0]  # Coefficient for similarity\n",
    "beta = lr.intercept_[0]  # Intercept\n",
    "\n",
    "print(f\"Calibration complete!\")\n",
    "print(f\"   Alpha (slope): {alpha:.4f}\")\n",
    "print(f\"   Beta (intercept): {beta:.4f}\")\n",
    "\n",
    "# Test calibration\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "test_similarities = similarities[:5].flatten()\n",
    "test_labels = ground_truths[:5]\n",
    "calibrated_probs = sigmoid(alpha * test_similarities + beta)\n",
    "\n",
    "print(f\"\\nCalibration test:\")\n",
    "for i in range(5):\n",
    "    print(f\"  Sim: {test_similarities[i]:.4f} → Prob: {calibrated_probs[i]:.4f}, True: {test_labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3e2d5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.transforms.transforms.Compose'>\n",
      "Compose(\n",
      "    RandomResizedCrop(size=(224, 224), scale=(0.9, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)\n",
      "    <function _convert_to_rgb at 0x7f3a9a191bc0>\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
      ")\n",
      "RGB\n",
      "RGB\n",
      "RGB\n",
      "RGB\n",
      "RGB\n"
     ]
    }
   ],
   "source": [
    "#investigate the preprocessing:\n",
    "\n",
    "print(type(preprocess))\n",
    "print(preprocess)\n",
    "\n",
    "# we can see the images are already in rgb mode, so no need to no that\n",
    "for pair in val_pairs[:5]:\n",
    "    image = load_image_from_pair(pair)\n",
    "    print(image.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "838c8abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking up the code for clip _convert_to_rgb it is:\n",
    "def _convert_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "# aka the built-in RGB conversion from PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c44228c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing BlackBox API...\n",
      "API Score: 0.7881 (Expected match: True)\n",
      "Queries used: 1/200\n",
      "\n",
      "Token edit cost example:\n",
      "  Original: 'A cat sitting on a mat'\n",
      "  Modified: 'A dog standing on a rug'\n",
      "  Cost: 3 token edits\n",
      "\n",
      "Pixel edit cost example:\n",
      "  Modified 50 pixels in 100x100 image\n"
     ]
    }
   ],
   "source": [
    "from scipy.ndimage import label\n",
    "\n",
    "# BlackBox API Implementation\n",
    "import editdistance  # For Levenshtein distance\n",
    "\n",
    "class BlackBoxAPI:\n",
    "    \"\"\"\n",
    "    Black-box API for the VLM scorer with query budget tracking and cost calculation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float, beta: float, q_max: int = 200):\n",
    "        \"\"\"\n",
    "        Initialize the black-box API.\n",
    "        \n",
    "        Args:\n",
    "            alpha: Logistic regression slope parameter\n",
    "            beta: Logistic regression intercept parameter  \n",
    "            q_max: Maximum queries allowed per sample\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.q_max = q_max\n",
    "        self.query_count = 0\n",
    "        \n",
    "    def score(self, image_uint8: np.ndarray, caption_str: str) -> float:\n",
    "        \"\"\"\n",
    "        Score image-caption pair and return probability.\n",
    "        \n",
    "        Args:\n",
    "            image_uint8: Image as uint8 numpy array (H, W, C)\n",
    "            caption_str: Caption string\n",
    "            \n",
    "        Returns:\n",
    "            Probability in [0, 1] using sigmoid(alpha * cosine + beta)\n",
    "        \"\"\"\n",
    "        if self.query_count >= self.q_max:\n",
    "            raise RuntimeError(f\"Query budget exceeded! Used {self.query_count}/{self.q_max}\")\n",
    "            \n",
    "        # Convert numpy array to PIL Image\n",
    "        image_pil = Image.fromarray(image_uint8)\n",
    "        \n",
    "        # Get cosine similarity\n",
    "        cosine_sim = clip_embed(image_pil, caption_str)\n",
    "        \n",
    "        # Apply calibrated sigmoid\n",
    "        logit = self.alpha * cosine_sim + self.beta\n",
    "        probability = 1 / (1 + np.exp(-logit))\n",
    "        \n",
    "        self.query_count += 1\n",
    "        \n",
    "        return probability\n",
    "    \n",
    "    def reset_query_count(self):\n",
    "        \"\"\"Reset query counter for new sample.\"\"\"\n",
    "        self.query_count = 0\n",
    "        \n",
    "    def get_remaining_queries(self) -> int:\n",
    "        \"\"\"Get remaining query budget.\"\"\"\n",
    "        return self.q_max - self.query_count\n",
    "\n",
    "# Cost Functions\n",
    "def token_edit_cost(original: str, modified: str) -> int:\n",
    "    \"\"\"\n",
    "    Compute token-level Levenshtein distance using CLIP tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        original: Original caption\n",
    "        modified: Modified caption\n",
    "        \n",
    "    Returns:\n",
    "        Number of token edits (insertions, deletions, substitutions)\n",
    "    \"\"\"\n",
    "    # Use CLIP tokenizer for more accurate tokenization\n",
    "    orig_tokens = open_clip.tokenize([original], context_length=77)[0].numpy()\n",
    "    mod_tokens = open_clip.tokenize([modified], context_length=77)[0].numpy()\n",
    "    \n",
    "    # Remove padding tokens (0s) and special tokens for fair comparison\n",
    "    # Keep only actual content tokens\n",
    "    orig_tokens = orig_tokens[orig_tokens != 0]\n",
    "    mod_tokens = mod_tokens[mod_tokens != 0]\n",
    "    \n",
    "    return editdistance.eval(orig_tokens.tolist(), mod_tokens.tolist())\n",
    "\n",
    "def pixel_edit_cost(original: np.ndarray, modified: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Compute number of changed pixels with reduced cost for continuous regions.\n",
    "    \n",
    "    Args:\n",
    "        original: Original image as uint8 numpy array\n",
    "        modified: Modified image as uint8 numpy array\n",
    "        \n",
    "    Returns:\n",
    "        Adjusted cost based on number of changed pixels, with reduced cost for continuous regions.\n",
    "    \"\"\"\n",
    "    # Find the difference mask\n",
    "    diff_mask = np.any(original != modified, axis=-1)\n",
    "    \n",
    "    # Label connected components in the difference mask\n",
    "    labeled_regions, num_features = label(diff_mask)\n",
    "    \n",
    "    # Count pixels in each connected region\n",
    "    total_cost = 0\n",
    "    for region_id in range(1, num_features + 1):\n",
    "        region_size = np.sum(labeled_regions == region_id)\n",
    "        if region_size > 0:\n",
    "            # Full cost for the first pixel, half cost for the rest\n",
    "            total_cost += 1 + (region_size - 1) * 0.5\n",
    "    \n",
    "    return int(total_cost)\n",
    "\n",
    "# Test the BlackBox API\n",
    "print(\"Testing BlackBox API...\")\n",
    "\n",
    "# Initialize API with calibrated parameters\n",
    "api = BlackBoxAPI(alpha, beta, q_max=200)\n",
    "\n",
    "# Test on a sample\n",
    "test_pair = val_pairs[0]\n",
    "test_image = load_image_from_pair(test_pair)\n",
    "test_image_uint8 = np.array(test_image)\n",
    "\n",
    "# Get score\n",
    "score = api.score(test_image_uint8, test_pair['caption'])\n",
    "print(f\"API Score: {score:.4f} (Expected match: {test_pair['is_match']})\")\n",
    "print(f\"Queries used: {api.query_count}/{api.q_max}\")\n",
    "\n",
    "# Test cost functions\n",
    "original_caption = \"A cat sitting on a mat\"\n",
    "modified_caption = \"A dog standing on a rug\" \n",
    "token_cost = token_edit_cost(original_caption, modified_caption)\n",
    "print(f\"\\nToken edit cost example:\")\n",
    "print(f\"  Original: '{original_caption}'\")  \n",
    "print(f\"  Modified: '{modified_caption}'\")\n",
    "print(f\"  Cost: {token_cost} token edits\")\n",
    "\n",
    "# Test pixel cost (create a simple modification)\n",
    "original_img = np.zeros((100, 100, 3), dtype=np.uint8)\n",
    "modified_img = original_img.copy()\n",
    "modified_img[10:20, 10:20] = 255  # Change a 10x10 region\n",
    "pixel_cost = pixel_edit_cost(original_img, modified_img)\n",
    "print(f\"\\nPixel edit cost example:\")\n",
    "print(f\"  Modified {pixel_cost} pixels in 100x100 image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26049a39",
   "metadata": {},
   "source": [
    "## Your Task: Implement Adversarial Attacks\n",
    "\n",
    "### Attack Function Template\n",
    "\n",
    "Replace the trivial baseline in the `attack()` function with sophisticated adversarial attacks:\n",
    "\n",
    "```python\n",
    "def attack(image_np_uint8, caption_str, api, budgets):\n",
    "    # Your attack implementation here!\n",
    "    # You can modify:\n",
    "    # - Caption tokens (text modifications)  \n",
    "    # - Image pixels (visual modifications)\n",
    "    # - Both (multimodal attack)\n",
    "    \n",
    "    # Stay within budgets:\n",
    "    # - budgets['T_MAX'] = 10  token edits\n",
    "    # - budgets['P_MAX'] = 100 pixel edits\n",
    "    # - budgets['Q_MAX'] = 100 queries\n",
    "    \n",
    "    return {\n",
    "        'success': success,      # bool: did you flip the decision?\n",
    "        'image': final_image,    # np.array: attacked image\n",
    "        'caption': final_caption, # str: attacked caption  \n",
    "        'token_cost': token_cost, # int: tokens changed\n",
    "        'pixel_cost': pixel_cost, # int: pixels changed\n",
    "        'query_cost': query_cost  # int: API calls made\n",
    "    }\n",
    "```\n",
    "\n",
    "### Attack Strategies to Consider\n",
    "\n",
    "- **Text Attacks**: Synonym replacement, word insertion/deletion, semantic paraphrasing\n",
    "- **Image Attacks**: Adversarial noise, targeted pixel modifications, patch attacks\n",
    "- **Query Optimization**: Gradient-free optimization, genetic algorithms, hill climbing\n",
    "- **Multimodal**: Combined text+image attacks for maximum effectiveness\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "Your attack will be scored as: **ASR - 0.5×ANC - 0.1×(AQ/Q_MAX)**\n",
    "\n",
    "- **ASR**: Attack Success Rate (higher is better)\n",
    "- **ANC**: Average Number of Changes (lower is better) \n",
    "- **AQ**: Average Queries (lower is better)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Implement your attack** in the `attack()` function above\n",
    "2. **Test locally** using the evaluation framework  \n",
    "3. **Run on full dataset** by changing `max_samples=None`\n",
    "\n",
    "Good luck! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb55729",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">**FILL THIS CODE BLOCK**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60b16376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, ToTensor, Normalize, Compose\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "\n",
    "def clip_loss(image_tensor, caption: str, label) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between image and text embeddings.\n",
    "    Returns a torch scalar that supports autograd.\n",
    "    \"\"\"\n",
    "    # convert to float\n",
    "    image_tensor = image_tensor.float().permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "    # Preprocess image (make sure to keep gradient tracking)\n",
    "    rrc = RandomResizedCrop(size=(224, 224), scale=(0.9, 1.0), ratio=(0.75, 1.3333), interpolation=InterpolationMode.BICUBIC, antialias=True)\n",
    "\n",
    "\n",
    "    norm = Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "    # Tokenize text (token embeddings are usually non-differentiable wrt raw string)   \n",
    "\n",
    "    image_tensor = rrc(image_tensor)\n",
    "    image_tensor = norm(image_tensor)\n",
    "\n",
    "\n",
    "    text_tokens = open_clip.tokenize([caption]).to(device)\n",
    "\n",
    "    # Get embeddings (these will remain differentiable wrt image_tensor)\n",
    "    image_features = model.encode_image(image_tensor)\n",
    "    text_features = model.encode_text(text_tokens)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    image_features = F.normalize(image_features, dim=-1)\n",
    "    text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "    similarity = (image_features @ text_features.T).squeeze()  # shape: scalar\n",
    "\n",
    "    output = similarity*alpha + beta\n",
    "    output = torch.sigmoid(output)\n",
    "    # Cosine similarity\n",
    "    if label: # if label is true\n",
    "        loss = 1 - output  # want to maximize similarity\n",
    "        \n",
    "    else: # if label is false\n",
    "        loss = output  # want to minimize similarity\n",
    "\n",
    "      \n",
    "    return loss\n",
    "\n",
    "def calc_image_grad(image_tensor, caption: str, label) -> torch.Tensor:\n",
    "    image_tensor = image_tensor.float().to(device)\n",
    "    image_tensor.requires_grad_(True)  # allow gradient wrt input\n",
    "    if image_tensor.grad is not None:\n",
    "        image_tensor.grad.zero_()\n",
    "    losses = []\n",
    "    for _ in range(20):\n",
    "        loss =  clip_loss(image_tensor, caption, label)\n",
    "        losses.append(loss)\n",
    "    total_loss = torch.stack(losses).mean()\n",
    "    image_grad = torch.autograd.grad(total_loss, [image_tensor])[0]\n",
    "    image_grad = image_grad.detach().squeeze().cpu()\n",
    "    return image_grad\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd8f5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def top_x(img: torch.Tensor, x: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Keep only the top x pixels (by L2 norm across channels) in a PyTorch image tensor.\n",
    "\n",
    "    This function is robust to both channel-first (C, H, W) and channel-last (H, W, C) tensors and\n",
    "    returns a masked tensor with the same layout as the input.\n",
    "\n",
    "    Args:\n",
    "        img (torch.Tensor): Tensor of shape (C, H, W) or (H, W, C).\n",
    "        x (int): Number of pixels to retain based on highest L2 norms.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Masked image tensor with only top-x pixels preserved (same layout as input).\n",
    "    \"\"\"\n",
    "\n",
    "    if img.ndim != 3:\n",
    "        raise ValueError(\"top_x expects a 3-D tensor (C,H,W) or (H,W,C)\")\n",
    "\n",
    "    # Detect layout: assume channel-first if first dim is typical channel count\n",
    "    if img.shape[0] in (1, 3, 4):\n",
    "        # (C, H, W)\n",
    "        C, H, W = img.shape\n",
    "        # norms per pixel across channels -> shape (H, W)\n",
    "        norms = torch.norm(img, p=2, dim=0)\n",
    "        flat = norms.view(-1)\n",
    "        k = min(int(x), flat.numel())\n",
    "        if k <= 0:\n",
    "            return torch.zeros_like(img)\n",
    "        topk_idx = torch.topk(flat, k, largest=True).indices\n",
    "        mask = torch.zeros_like(flat)\n",
    "        mask[topk_idx] = 1.0\n",
    "        mask = mask.view(H, W)\n",
    "        masked = img * mask.unsqueeze(0)\n",
    "        return masked\n",
    "    else:\n",
    "        # Assume (H, W, C)\n",
    "        H, W, C = img.shape\n",
    "        norms = torch.norm(img, p=2, dim=2)  # (H, W)\n",
    "        flat = norms.view(-1)\n",
    "        k = min(int(x), flat.numel())\n",
    "        if k <= 0:\n",
    "            return torch.zeros_like(img)\n",
    "        topk_idx = torch.topk(flat, k, largest=True).indices\n",
    "        mask = torch.zeros_like(flat)\n",
    "        mask[topk_idx] = 1.0\n",
    "        mask = mask.view(H, W).unsqueeze(2)\n",
    "        masked = img * mask\n",
    "        return masked, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e583cbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "loss: 0.5400, Similarity: 0.3342, Match: True\n",
      "Adversarial loss: 0.5401, Adversarial Similarity: 0.3342\n",
      "\n",
      "Sample 2:\n",
      "loss: 0.4119, Similarity: 0.0343, Match: False\n",
      "Adversarial loss: 0.4122, Adversarial Similarity: 0.0343\n",
      "\n",
      "Sample 3:\n",
      "loss: 0.3687, Similarity: 0.0719, Match: False\n",
      "Adversarial loss: 0.3688, Adversarial Similarity: 0.0720\n",
      "\n",
      "Sample 4:\n",
      "loss: 0.5568, Similarity: 0.1899, Match: False\n",
      "Adversarial loss: 0.5573, Adversarial Similarity: 0.1899\n",
      "\n",
      "Sample 5:\n",
      "loss: 0.4793, Similarity: 0.3785, Match: True\n",
      "Adversarial loss: 0.4794, Adversarial Similarity: 0.3785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:49<00:00, 10.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average test loss: 0.4605\n",
      "Average adversarial loss: 0.4607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def image_attack(image: np.array, caption: str, label: bool, pixels_changed = 100, epsilon = 10, steps: int = 10) -> np.array:\n",
    "\n",
    "    image = image.astype(np.uint8)\n",
    "    # Work on float copies\n",
    "    original = torch.tensor(image.astype(np.float32))\n",
    "    adversarial_image = torch.tensor(image.copy().astype(np.float32))\n",
    "    step_eps = float(epsilon) / max(1, steps)\n",
    "\n",
    "    # Iterative gradient steps (unconstrained)\n",
    "    for _ in range(steps):\n",
    "        image_grad = calc_image_grad(adversarial_image, caption, label).detach()\n",
    "        adversarial_image.requires_grad_(False)\n",
    "        # apply small step everywhere (not sparsified here) \n",
    "        step = step_eps * image_grad.sign()\n",
    "        adversarial_image += step\n",
    "        adversarial_image = torch.clip(adversarial_image, 0, 255)\n",
    "\n",
    "    # Compute full perturbation\n",
    "    perturb = adversarial_image - original  # float32 HWC\n",
    "\n",
    "    perturb, mask = top_x(torch.tensor(perturb), pixels_changed)\n",
    "\n",
    "\n",
    "    # Apply masked perturbation to original \n",
    "    perturb = perturb.numpy()\n",
    "    final = np.clip(image + perturb.astype(np.uint8), 0, 255)\n",
    "\n",
    "    return final.astype(np.uint8)\n",
    "\n",
    "\n",
    "\n",
    "for nr, pair in enumerate(val_pairs[:5]):\n",
    "    image = load_image_from_pair(pair)\n",
    "    image_tensor = torch.tensor(np.array(image))  # Change to (C, H, W)\n",
    "    # this is how it should be done according to the preprocess function\n",
    "    # but its kinda wrong since the preprocess function is maybe for training\n",
    "    # idk maybe we can find it out later, so the gradient can be passed through it\n",
    "    #image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "    #print(image_tensor)\n",
    "    torch.manual_seed(42)\n",
    "    loss = clip_loss(image_tensor, pair['caption'], pair['is_match'])\n",
    "    torch.manual_seed(42)\n",
    "    similarity = clip_embed(image, pair['caption'])\n",
    "    print(f\"\\nSample {nr+1}:\")\n",
    "    print(f\"loss: {loss.item():.4f}, Similarity: {similarity:.4f}, Match: {pair['is_match']}\")\n",
    "    \n",
    "    epsilon = 2# step size for adversarial perturbation\n",
    "    adversarial_image_np = image_attack(np.array(image), pair['caption'], pair['is_match'], pixels_changed=100, epsilon=epsilon, steps=15)\n",
    "\n",
    "    #reconvert tensor to image\n",
    "    adversarial_image = PIL.Image.fromarray(adversarial_image_np)\n",
    "    torch.manual_seed(42)\n",
    "    adversarial_loss = clip_loss(torch.Tensor(adversarial_image_np), pair['caption'], pair['is_match'])\n",
    "    torch.manual_seed(42)\n",
    "    adversarial_similarity = clip_embed(adversarial_image, pair['caption'])\n",
    "\n",
    "    print(f\"Adversarial loss: {adversarial_loss.item():.4f}, Adversarial Similarity: {adversarial_similarity:.4f}\")\n",
    "\n",
    "test_losses = []\n",
    "adversarial_losses = []\n",
    "\n",
    "for pair in tqdm(val_pairs[:10]):\n",
    "    image = load_image_from_pair(pair)\n",
    "    image_tensor = torch.tensor(np.array(image))  # Change to (C, H, W)\n",
    "    # this is how it should be done according to the preprocess function\n",
    "    # but its kinda wrong since the preprocess function is maybe for training\n",
    "    # idk maybe we can find it out later, so the gradient can be passed through it\n",
    "    #image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "    #print(image_tensor)\n",
    "    torch.manual_seed(42)\n",
    "    loss = clip_loss(image_tensor, pair['caption'], pair['is_match'])\n",
    "\n",
    "    similarity = clip_embed(image, pair['caption'])\n",
    "    #print(f\"\\nSample {nr+1}:\")\n",
    "    #print(f\"loss: {loss.item():.4f}, Similarity: {similarity:.4f}, Match: {pair['is_match']}\")\n",
    "    \n",
    "    epsilon = 2# step size for adversarial perturbation\n",
    "    adversarial_image_np = image_attack(np.array(image), pair['caption'], pair['is_match'], pixels_changed=100, epsilon=epsilon, steps=15)\n",
    "\n",
    "    #reconvert tensor to image\n",
    "    adversarial_image = PIL.Image.fromarray(adversarial_image_np)\n",
    "    torch.manual_seed(42)\n",
    "    adversarial_loss = clip_loss(torch.Tensor(adversarial_image_np), pair['caption'], pair['is_match'])\n",
    "    torch.manual_seed(42)\n",
    "    adversarial_similarity = clip_embed(adversarial_image, pair['caption'])\n",
    "\n",
    "    test_losses.append(loss.item())\n",
    "    adversarial_losses.append(adversarial_loss.item())\n",
    "\n",
    "print(f\"\\nAverage test loss: {np.mean(test_losses):.4f}\")\n",
    "print(f\"Average adversarial loss: {np.mean(adversarial_losses):.4f}\")\n",
    "\n",
    "\n",
    "# no limit\n",
    "#Average test loss: 0.4674\n",
    "#Average adversarial loss: 0.6009\n",
    "\n",
    "# 100 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14280182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:03<00:00, 12.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average test loss: 0.4605\n",
      "Average adversarial loss: 0.4654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def image_attack_standalone(image: np.ndarray,\n",
    "                            caption: str,\n",
    "                            label,\n",
    "                            pixels_changed: int = 100,\n",
    "                            epsilon: float = 10.0,\n",
    "                            dense_steps: int = 10,\n",
    "                            reopt_steps: int = 300,\n",
    "                            reopt_lr: float = 0.03,\n",
    "                            use_iterations: bool = False,\n",
    "                            device: torch.device = None):\n",
    "    \"\"\"\n",
    "    Two-stage sparse L0 attack with optimized dense-stage:\n",
    "      - Default (use_iterations=False): one-shot gradient importance to pick top-k pixels (fast)\n",
    "      - If use_iterations=True: a few gradient-following raw steps without sign (slower, stronger)\n",
    "    Then re-optimize values on the selected support using clip_loss + Adam.\n",
    "\n",
    "    Requires:\n",
    "      - calc_image_grad(image_tensor, caption, label): returns gradient dL/dx shaped H x W x C\n",
    "      - top_x(perturb_tensor, k) [optional]: returns (sparse_perturb, mask) OR you can rely on internal selection\n",
    "      - clip_loss(image_tensor, caption, label): returns a scalar torch.Tensor (autograd-compatible)\n",
    "    \"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --------- prepare tensors ----------\n",
    "    img_np = image.astype(np.uint8)\n",
    "    H, W, C = img_np.shape\n",
    "\n",
    "    orig = torch.tensor(img_np.astype(np.float32), device=device)   # H x W x C\n",
    "    orig_bchw = orig.permute(2, 0, 1).unsqueeze(0).contiguous()      # 1 x C x H x W\n",
    "\n",
    "    # --------- Dense stage: compute importance (two options) ----------\n",
    "    if not use_iterations:\n",
    "        # Option A: one-shot gradient at original (fast)\n",
    "        grad_hwc = calc_image_grad(orig, caption, label).detach().to(device)  # H x W x C\n",
    "        # per-pixel importance: L2 across channels\n",
    "        importance = torch.sqrt(torch.sum(grad_hwc * grad_hwc, dim=2))     # H x W\n",
    "        # We'll use the gradient values later to initialize delta on the selected support.\n",
    "        dense_grad_hwc = grad_hwc\n",
    "    else:\n",
    "        # Option B: a few raw gradient-following steps (no sign). This may give a better dense perturbation.\n",
    "        adv = orig.clone().detach()\n",
    "        step_eps = float(epsilon) / max(1, dense_steps)\n",
    "        for _ in range(dense_steps):\n",
    "            g = calc_image_grad(adv, caption, label).detach().to(device)    # H x W x C\n",
    "            gnorm = torch.sqrt(torch.sum(g**2)) + 1e-12\n",
    "            adv = torch.clamp(adv + (step_eps * g / gnorm), 0.0, 255.0)\n",
    "        dense_perturb = (adv - orig).detach()    # H x W x C\n",
    "        importance = torch.sqrt(torch.sum(dense_perturb**2, dim=2))  # H x W\n",
    "        # convert to \"grad-like\" values for delta init:\n",
    "        dense_grad_hwc = dense_perturb\n",
    "\n",
    "    # --------- select top-k pixels by importance ----------\n",
    "    flat = importance.view(-1)\n",
    "    k = min(pixels_changed, flat.numel())\n",
    "    topk = torch.topk(flat, k)\n",
    "    topk_indices = topk.indices  # indices into flattened H*W\n",
    "\n",
    "    # build mask H x W (float 0/1)\n",
    "    mask_hw = torch.zeros_like(flat, device=device)\n",
    "    mask_hw[topk_indices] = 1.0\n",
    "    mask_hw = mask_hw.view(H, W)   # H x W\n",
    "\n",
    "    # convert mask to 1 x C x H x W (same shape as image tensor)\n",
    "    mask_bchw = mask_hw.unsqueeze(0).unsqueeze(0).repeat(1, C, 1, 1).float()\n",
    "\n",
    "\n",
    "    # --------- build initial sparse perturbation (delta) using gradient info ----------\n",
    "    # Convert dense_grad_hwc (H x W x C) to B x C x H x W\n",
    "    dense_grad_bchw = dense_grad_hwc.permute(2, 0, 1).unsqueeze(0).contiguous()  # 1 x C x H x W\n",
    "\n",
    "    # Initialize delta on the support as a small scaled step in gradient direction.\n",
    "    # Using epsilon as total allowed amplitude scale (you can tune factor)\n",
    "    init_scale = float(epsilon)  # scale factor for initial delta; you can reduce (e.g., epsilon*0.5)\n",
    "    delta_init = (init_scale * dense_grad_bchw) * mask_bchw  # only on mask\n",
    "    # Optionally clip initial delta to reasonable bounds so that orig + delta stays in [0,255]\n",
    "    with torch.no_grad():\n",
    "        delta_init = torch.clamp(orig_bchw + delta_init, 0.0, 255.0) - orig_bchw\n",
    "\n",
    "    delta = delta_init.clone().detach().to(device)\n",
    "    delta.requires_grad_(True)\n",
    "\n",
    "    # --------- re-optimize values on support using Adam and clip_loss ----------\n",
    "    optimizer = torch.optim.Adam([delta], lr=reopt_lr)\n",
    "\n",
    "    # Decide whether to maximize clip_loss or minimize (common: maximize model loss for untargeted)\n",
    "    maximize = True\n",
    "\n",
    "    for it in range(reopt_steps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        adv_bchw = orig_bchw + delta * mask_bchw   # only masked positions change\n",
    "        adv_hwc = adv_bchw.squeeze(0).permute(1, 2, 0).contiguous()  # H x W x C\n",
    "\n",
    "        loss_val = clip_loss(adv_hwc, caption, label)\n",
    "        if not isinstance(loss_val, torch.Tensor):\n",
    "            # make it a torch scalar (no grad) — preferably clip_loss is autograd-capable\n",
    "            loss_val = torch.tensor(float(loss_val), device=device, requires_grad=False)\n",
    "\n",
    "        # maximize clip_loss if requested (backprop on -loss)\n",
    "        loss_to_backprop = -loss_val if maximize else loss_val\n",
    "        # If clip_loss is not autograd-compatible, you'd need to set delta.grad manually via calc_image_grad (see note below)\n",
    "        loss_to_backprop.backward()\n",
    "\n",
    "        # Mask the gradients so only selected pixels are updated.\n",
    "        if delta.grad is not None:\n",
    "            delta.grad.data *= mask_bchw\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Keep adv in valid range and update delta = adv - orig\n",
    "        with torch.no_grad():\n",
    "            adv_proj = torch.clamp(orig_bchw + delta * mask_bchw, 0.0, 255.0)\n",
    "            delta.data = (adv_proj - orig_bchw)\n",
    "\n",
    "        # (optional) early stopping: you can break if clip_loss indicates success\n",
    "        # e.g., if targeted: break when model predicts target; if untargeted: break when misclassified.\n",
    "\n",
    "    # --------- finalize adversarial image ----------\n",
    "    final_adv_bchw = torch.clamp(orig_bchw + delta * mask_bchw, 0.0, 255.0).squeeze(0)\n",
    "    final_adv_hwc = final_adv_bchw.permute(1, 2, 0).contiguous()\n",
    "    final_adv_np = final_adv_hwc.cpu().detach().numpy().astype(np.uint8)\n",
    "\n",
    "    return final_adv_np\n",
    "\n",
    "\n",
    "test_losses = []\n",
    "adversarial_losses = []\n",
    "\n",
    "for pair in tqdm(val_pairs[:10]):\n",
    "    image = load_image_from_pair(pair)\n",
    "    image_tensor = torch.tensor(np.array(image))  # Change to (C, H, W)\n",
    "    # this is how it should be done according to the preprocess function\n",
    "    # but its kinda wrong since the preprocess function is maybe for training\n",
    "    # idk maybe we can find it out later, so the gradient can be passed through it\n",
    "    #image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "    #print(image_tensor)\n",
    "    torch.manual_seed(42)\n",
    "    loss = clip_loss(image_tensor, pair['caption'], pair['is_match'])\n",
    "\n",
    "    similarity = clip_embed(image, pair['caption'])\n",
    "    #print(f\"\\nSample {nr+1}:\")\n",
    "    #print(f\"loss: {loss.item():.4f}, Similarity: {similarity:.4f}, Match: {pair['is_match']}\")\n",
    "    \n",
    "    epsilon = 2# step size for adversarial perturbation\n",
    "    adversarial_image_np = image_attack_standalone(np.array(image), pair['caption'], pair['is_match'], pixels_changed=100, epsilon=epsilon)\n",
    "\n",
    "    #reconvert tensor to image\n",
    "    adversarial_image = PIL.Image.fromarray(adversarial_image_np)\n",
    "    torch.manual_seed(42)\n",
    "    adversarial_loss = clip_loss(torch.Tensor(adversarial_image_np), pair['caption'], pair['is_match'])\n",
    "    torch.manual_seed(42)\n",
    "    adversarial_similarity = clip_embed(adversarial_image, pair['caption'])\n",
    "\n",
    "    test_losses.append(loss.item())\n",
    "    adversarial_losses.append(adversarial_loss.item())\n",
    "\n",
    "print(f\"\\nAverage test loss: {np.mean(test_losses):.4f}\")\n",
    "print(f\"Average adversarial loss: {np.mean(adversarial_losses):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "088675d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack function defined (TRIVIAL BASELINE)\n",
      "   Students should replace the trivial implementation with sophisticated attacks!\n",
      "   Current baseline: Returns original inputs unchanged (0% success rate expected)\n",
      "\n",
      "Testing attack function...\n"
     ]
    }
   ],
   "source": [
    "def attack(image_np_uint8: np.ndarray, caption_str: str, api: BlackBoxAPI, budgets: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Student attack function to implement adversarial attacks.\n",
    "    \n",
    "    Args:\n",
    "        image_np_uint8: Original image as uint8 numpy array (H, W, C)\n",
    "        caption_str: Original caption string\n",
    "        api: BlackBoxAPI instance for querying the model\n",
    "        budgets: Dictionary with 'T_MAX', 'P_MAX', 'Q_MAX' limits\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - 'success': bool, whether attack succeeded (flipped decision)\n",
    "        - 'image': np.ndarray, final attacked image  \n",
    "        - 'caption': str, final attacked caption\n",
    "        - 'token_cost': int, number of token edits used\n",
    "        - 'pixel_cost': int, number of pixel edits used\n",
    "        - 'query_cost': int, number of queries used\n",
    "    \"\"\"\n",
    "    \n",
    "    # TRIVIAL BASELINE - Students should replace this!\n",
    "    # This baseline just returns the original inputs without any attack\n",
    "    \n",
    "    original_image = image_np_uint8.copy()\n",
    "    original_caption = caption_str\n",
    "    \n",
    "    \n",
    "    # Get original score to determine target (flip the decision)\n",
    "    original_score = api.score(original_image, original_caption)\n",
    "    \n",
    "\n",
    "    # For this baseline, we don't actually perform any attack\n",
    "    # Students should implement sophisticated attacks here!\n",
    "    \n",
    "    # TODO: IMPLEMENT YOUR ATTACK HERE!\n",
    "    # You can modify:\n",
    "    # - Caption tokens (text modifications)  \n",
    "    # - Image pixels (visual modifications)\n",
    "    # - Both (multimodal attack)\n",
    "    \n",
    "    # Stay within budgets:\n",
    "    # - budgets['T_MAX'] = maximum token edits\n",
    "    # - budgets['P_MAX'] = maximum pixel edits\n",
    "    # - budgets['Q_MAX'] = maximum queries\n",
    "    \n",
    "    # Attack Strategies to Consider:\n",
    "    # - Text Attacks: Synonym replacement, word insertion/deletion, semantic paraphrasing\n",
    "    # - Image Attacks: Adversarial noise, targeted pixel modifications, patch attacks\n",
    "    # - Query Optimization: Gradient-free optimization, genetic algorithms, hill climbing\n",
    "    # - Multimodal: Combined text+image attacks for maximum effectiveness\n",
    "\n",
    "    original_label = original_score > 0.5\n",
    "\n",
    "    #final_image = image_attack(original_image, original_caption, original_label, pixels_changed=100, epsilon=2, steps=15)        # Comment out this line when implementing image attacks\n",
    "    final_image = image_attack_standalone(original_image, original_caption, original_label, pixels_changed=100, epsilon=2)  # Comment out this line when implementing image attacks\n",
    "    final_caption = original_caption    # Comment out this line when implementing text attacks\n",
    "    \n",
    "    # Get final score  \n",
    "    final_score = api.score(final_image, final_caption)\n",
    "    \n",
    "    # Check if attack succeeded (decision flipped)\n",
    "    original_decision = original_score > 0.5\n",
    "    final_decision = final_score > 0.5\n",
    "    success = (original_decision != final_decision)\n",
    "    \n",
    "    # Calculate costs\n",
    "    token_cost = token_edit_cost(original_caption, final_caption)\n",
    "    pixel_cost = pixel_edit_cost(original_image, final_image)\n",
    "    query_cost = api.query_count\n",
    "    \n",
    "    return {\n",
    "        'success': success,\n",
    "        'image': final_image,\n",
    "        'caption': final_caption,\n",
    "        'token_cost': token_cost,\n",
    "        'pixel_cost': pixel_cost,  \n",
    "        'query_cost': query_cost,\n",
    "        'original_score': original_score,\n",
    "        'final_score': final_score\n",
    "    }\n",
    "\n",
    "print(\"Attack function defined (TRIVIAL BASELINE)\")\n",
    "print(\"   Students should replace the trivial implementation with sophisticated attacks!\")\n",
    "print(\"   Current baseline: Returns original inputs unchanged (0% success rate expected)\")\n",
    "\n",
    "# Test the attack function\n",
    "print(\"\\nTesting attack function...\")\n",
    "test_pair = val_pairs[0]\n",
    "test_image = np.array(load_image_from_pair(test_pair))\n",
    "\n",
    "# Create fresh API instance  \n",
    "test_api = BlackBoxAPI(alpha, beta, q_max=100)\n",
    "\n",
    "attack_budgets = {\n",
    "    'T_MAX': 10,     # Maximum token edits per sample\n",
    "    'P_MAX': 100,   # Maximum pixel edits per sample  \n",
    "    'Q_MAX': 100    # Maximum queries per sample\n",
    "}\n",
    "\n",
    "result = attack(test_image, test_pair['caption'], test_api, attack_budgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7a10633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on validation set...\n",
      "Running on first 50 samples for quick testing...\n",
      "Starting evaluation...\n",
      "Evaluating on 50 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attacking: 100%|██████████| 50/50 [09:46<00:00, 11.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EVALUATION RESULTS (50 samples):\n",
      "==================================================\n",
      "Attack Success Rate (ASR): 0.0%\n",
      "Average Number of Changes (ANC): 52.90\n",
      "Average Queries (AQ): 2.0\n",
      "Final Score: -0.1343\n",
      "==================================================\n",
      "Budget Usage:\n",
      "  Avg Token Cost: 0.00/10\n",
      "  Avg Pixel Cost: 52.90/100\n",
      "  Avg Query Cost: 2.0/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Framework\n",
    "def evaluate_attack(val_pairs: list, attack_function, alpha: float, beta: float, budgets: dict, max_samples: int = None):\n",
    "    \"\"\"\n",
    "    Evaluate attack function on validation pairs.\n",
    "    \n",
    "    Args:\n",
    "        val_pairs: List of validation pairs\n",
    "        attack_function: Attack function to evaluate\n",
    "        alpha, beta: Calibrated parameters\n",
    "        budgets: Attack budgets dictionary\n",
    "        max_samples: Limit number of samples (None = all)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting evaluation...\")\n",
    "    \n",
    "    # Limit samples if specified\n",
    "    eval_pairs = val_pairs[:max_samples] if max_samples else val_pairs\n",
    "    print(f\"Evaluating on {len(eval_pairs)} samples\")\n",
    "    \n",
    "    results = []\n",
    "    total_success = 0\n",
    "    total_token_cost = 0\n",
    "    total_pixel_cost = 0\n",
    "    total_query_cost = 0\n",
    "    \n",
    "    for i, pair in enumerate(tqdm(eval_pairs, desc=\"Attacking\")):\n",
    "        # Create fresh API instance for each sample\n",
    "        api = BlackBoxAPI(alpha, beta, q_max=budgets['Q_MAX'])\n",
    "        \n",
    "        # Load image\n",
    "        image = np.array(load_image_from_pair(pair))\n",
    "        caption = pair['caption']\n",
    "        \n",
    "        try:\n",
    "            # Run attack\n",
    "            result = attack_function(image, caption, api, budgets)\n",
    "            \n",
    "            # Validate budget constraints\n",
    "            budget_valid = (\n",
    "                result['token_cost'] <= budgets['T_MAX'] and\n",
    "                result['pixel_cost'] <= budgets['P_MAX'] and  \n",
    "                result['query_cost'] <= budgets['Q_MAX']\n",
    "            )\n",
    "            \n",
    "            if not budget_valid:\n",
    "                print(f\"Sample {i}: Budget violation!\")\n",
    "                print(f\"   Tokens: {result['token_cost']}/{budgets['T_MAX']}\")\n",
    "                print(f\"   Pixels: {result['pixel_cost']}/{budgets['P_MAX']}\")  \n",
    "                print(f\"   Queries: {result['query_cost']}/{budgets['Q_MAX']}\")\n",
    "                result['success'] = False  # Invalid attacks count as failures\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            if result['success']:\n",
    "                total_success += 1\n",
    "            total_token_cost += result['token_cost']\n",
    "            total_pixel_cost += result['pixel_cost']\n",
    "            total_query_cost += result['query_cost']\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Sample {i}: Attack failed with error: {e}\")\n",
    "            # Add failed result\n",
    "            results.append({\n",
    "                'success': False,\n",
    "                'token_cost': budgets['T_MAX'],  # Penalize failures\n",
    "                'pixel_cost': budgets['P_MAX'], \n",
    "                'query_cost': budgets['Q_MAX'],\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    n_samples = len(results)\n",
    "    asr = total_success / n_samples  # Attack Success Rate\n",
    "    anc = (10*total_token_cost + total_pixel_cost) / n_samples  # Average Number of Changes  \n",
    "    aq = total_query_cost / n_samples  # Average Queries\n",
    "    \n",
    "    # Final score: ASR - 0.5*ANC - 0.1*(AQ/Q_MAX)\n",
    "    score = asr - 0.5 * (anc / (10*budgets['T_MAX'] + budgets['P_MAX'])) - 0.1 * (aq / budgets['Q_MAX'])\n",
    "    \n",
    "    evaluation_result = {\n",
    "        'ASR': asr,\n",
    "        'ANC': anc, \n",
    "        'AQ': aq,\n",
    "        'Score': score,\n",
    "        'n_samples': n_samples,\n",
    "        'total_success': total_success,\n",
    "        'avg_token_cost': total_token_cost / n_samples,\n",
    "        'avg_pixel_cost': total_pixel_cost / n_samples,\n",
    "        'budgets': budgets,\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    return evaluation_result\n",
    "\n",
    "# Run Evaluation\n",
    "print(\"Running evaluation on validation set...\")\n",
    "\n",
    "# Define attack budgets\n",
    "attack_budgets = {\n",
    "    'T_MAX': 10,     # Maximum token edits per sample\n",
    "    'P_MAX': 100,   # Maximum pixel edits per sample  \n",
    "    'Q_MAX': 100    # Maximum queries per sample\n",
    "}\n",
    "\n",
    "# Evaluate on subset first (faster for testing)\n",
    "print(\"Running on first 50 samples for quick testing...\")\n",
    "eval_result = evaluate_attack(\n",
    "    val_pairs=val_pairs, \n",
    "    attack_function=attack,\n",
    "    alpha=alpha,\n",
    "    beta=beta, \n",
    "    budgets=attack_budgets,\n",
    "    max_samples=50  # Quick test on 50 samples\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nEVALUATION RESULTS (50 samples):\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Attack Success Rate (ASR): {eval_result['ASR']:.1%}\")\n",
    "print(f\"Average Number of Changes (ANC): {eval_result['ANC']:.2f}\")  \n",
    "print(f\"Average Queries (AQ): {eval_result['AQ']:.1f}\")\n",
    "print(f\"Final Score: {eval_result['Score']:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Budget Usage:\")\n",
    "print(f\"  Avg Token Cost: {eval_result['avg_token_cost']:.2f}/{attack_budgets['T_MAX']}\")\n",
    "print(f\"  Avg Pixel Cost: {eval_result['avg_pixel_cost']:.2f}/{attack_budgets['P_MAX']}\")  \n",
    "print(f\"  Avg Query Cost: {eval_result['AQ']:.1f}/{attack_budgets['Q_MAX']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a906503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running FULL evaluation on all 1000 validation samples...\n",
      "This may take several minutes depending on your attack implementation.\n",
      "Starting evaluation...\n",
      "Evaluating on 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attacking: 100%|██████████| 1000/1000 [05:18<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL EVALUATION RESULTS:\n",
      "============================================================\n",
      "Attack Success Rate (ASR): 0.2%\n",
      "Average Number of Changes (ANC): 76.19\n",
      "Average Queries (AQ): 2.0\n",
      "Final Score: -0.1905\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Full Evaluation (Uncomment when ready to test your attack)\n",
    "\n",
    "def run_full_evaluation():\n",
    "    \"\"\"Run evaluation on all 1000 validation samples.\"\"\"\n",
    "    print(\"Running FULL evaluation on all 1000 validation samples...\")\n",
    "    print(\"This may take several minutes depending on your attack implementation.\")\n",
    "    \n",
    "    full_result = evaluate_attack(\n",
    "        val_pairs=val_pairs,\n",
    "        attack_function=attack, \n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        budgets=attack_budgets,\n",
    "        max_samples=None  # All samples\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFINAL EVALUATION RESULTS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Attack Success Rate (ASR): {full_result['ASR']:.1%}\")\n",
    "    print(f\"Average Number of Changes (ANC): {full_result['ANC']:.2f}\")\n",
    "    print(f\"Average Queries (AQ): {full_result['AQ']:.1f}\")\n",
    "    print(f\"Final Score: {full_result['Score']:.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return full_result\n",
    "\n",
    "# Uncomment the line below when ready to run full evaluation:\n",
    "full_results = run_full_evaluation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41963ede",
   "metadata": {},
   "source": [
    "## Baseline scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0498a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_attack(image_np_uint8: np.ndarray, caption_str: str, api: BlackBoxAPI, budgets: dict) -> dict:\n",
    "\n",
    "    # TRIVIAL BASELINE - Students should replace this!\n",
    "    # This baseline just returns the original inputs without any attack\n",
    "    \n",
    "    original_image = image_np_uint8.copy()\n",
    "    original_caption = caption_str\n",
    "    \n",
    "    # Get original score to determine target (flip the decision)\n",
    "    original_score = api.score(original_image, original_caption)\n",
    "\n",
    "    final_image = original_image        # Comment out this line when implementing image attacks\n",
    "    final_caption = original_caption    # Comment out this line when implementing text attacks\n",
    "    \n",
    "    # Get final score  \n",
    "    final_score = api.score(final_image, final_caption)\n",
    "    \n",
    "    # Check if attack succeeded (decision flipped)\n",
    "    original_decision = original_score > 0.5\n",
    "    final_decision = final_score > 0.5\n",
    "    success = (original_decision != final_decision)\n",
    "    \n",
    "    # Calculate costs\n",
    "    token_cost = token_edit_cost(original_caption, final_caption)\n",
    "    pixel_cost = pixel_edit_cost(original_image, final_image)\n",
    "    query_cost = api.query_count\n",
    "    \n",
    "    return {\n",
    "        'success': success,\n",
    "        'image': final_image,\n",
    "        'caption': final_caption,\n",
    "        'token_cost': token_cost,\n",
    "        'pixel_cost': pixel_cost,  \n",
    "        'query_cost': query_cost,\n",
    "        'original_score': original_score,\n",
    "        'final_score': final_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f15b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on validation set...\n",
      "Running on first 50 samples for quick testing...\n",
      "Starting evaluation...\n",
      "Evaluating on 50 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attacking: 100%|██████████| 50/50 [00:01<00:00, 28.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EVALUATION RESULTS (50 samples):\n",
      "==================================================\n",
      "Attack Success Rate (ASR): 0.0%\n",
      "Average Number of Changes (ANC): 0.00\n",
      "Average Queries (AQ): 2.0\n",
      "Final Score: -0.0020\n",
      "==================================================\n",
      "Budget Usage:\n",
      "  Avg Token Cost: 0.00/10\n",
      "  Avg Pixel Cost: 0.00/100\n",
      "  Avg Query Cost: 2.0/100\n",
      "\n",
      "NOTE: This is a trivial baseline (0% ASR expected)\n",
      "Students should implement sophisticated attacks to improve ASR!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Run Evaluation\n",
    "print(\"Running evaluation on validation set...\")\n",
    "\n",
    "# Define attack budgets\n",
    "attack_budgets = {\n",
    "    'T_MAX': 10,     # Maximum token edits per sample\n",
    "    'P_MAX': 100,   # Maximum pixel edits per sample  \n",
    "    'Q_MAX': 100    # Maximum queries per sample\n",
    "}\n",
    "\n",
    "# Evaluate on subset first (faster for testing)\n",
    "print(\"Running on first 50 samples for quick testing...\")\n",
    "eval_result = evaluate_attack(\n",
    "    val_pairs=val_pairs, \n",
    "    attack_function=baseline_attack,\n",
    "    alpha=alpha,\n",
    "    beta=beta, \n",
    "    budgets=attack_budgets,\n",
    "    max_samples=50  # Quick test on 50 samples\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nEVALUATION RESULTS (50 samples):\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Attack Success Rate (ASR): {eval_result['ASR']:.1%}\")\n",
    "print(f\"Average Number of Changes (ANC): {eval_result['ANC']:.2f}\")  \n",
    "print(f\"Average Queries (AQ): {eval_result['AQ']:.1f}\")\n",
    "print(f\"Final Score: {eval_result['Score']:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Budget Usage:\")\n",
    "print(f\"  Avg Token Cost: {eval_result['avg_token_cost']:.2f}/{attack_budgets['T_MAX']}\")\n",
    "print(f\"  Avg Pixel Cost: {eval_result['avg_pixel_cost']:.2f}/{attack_budgets['P_MAX']}\")  \n",
    "print(f\"  Avg Query Cost: {eval_result['AQ']:.1f}/{attack_budgets['Q_MAX']}\")\n",
    "print(f\"\\nNOTE: This is a trivial baseline (0% ASR expected)\")\n",
    "print(f\"Students should implement sophisticated attacks to improve ASR!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307d8f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running FULL evaluation on all 1000 validation samples...\n",
      "This may take several minutes depending on your attack implementation.\n",
      "Starting evaluation...\n",
      "Evaluating on 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attacking: 100%|██████████| 1000/1000 [00:34<00:00, 28.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL EVALUATION RESULTS:\n",
      "============================================================\n",
      "Attack Success Rate (ASR): 0.6%\n",
      "Average Number of Changes (ANC): 0.00\n",
      "Average Queries (AQ): 2.0\n",
      "Final Score: 0.0040\n",
      "============================================================\n",
      "To run full evaluation on all 1000 samples:\n",
      "Uncomment: full_results = run_full_evaluation()\n",
      "\n",
      "Current status: Framework ready for student implementations!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Full Evaluation (Uncomment when ready to test your attack)\n",
    "\n",
    "def run_full_evaluation():\n",
    "    \"\"\"Run evaluation on all 1000 validation samples.\"\"\"\n",
    "    print(\"Running FULL evaluation on all 1000 validation samples...\")\n",
    "    print(\"This may take several minutes depending on your attack implementation.\")\n",
    "    \n",
    "    full_result = evaluate_attack(\n",
    "        val_pairs=val_pairs,\n",
    "        attack_function=baseline_attack, \n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        budgets=attack_budgets,\n",
    "        max_samples=None  # All samples\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFINAL EVALUATION RESULTS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Attack Success Rate (ASR): {full_result['ASR']:.1%}\")\n",
    "    print(f\"Average Number of Changes (ANC): {full_result['ANC']:.2f}\")\n",
    "    print(f\"Average Queries (AQ): {full_result['AQ']:.1f}\")\n",
    "    print(f\"Final Score: {full_result['Score']:.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return full_result\n",
    "\n",
    "# Uncomment the line below when ready to run full evaluation:\n",
    "full_results = run_full_evaluation()\n",
    "\n",
    "print(\"To run full evaluation on all 1000 samples:\")\n",
    "print(\"Uncomment: full_results = run_full_evaluation()\")\n",
    "print(\"\\nCurrent status: Framework ready for student implementations!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
