{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0666fbb",
   "metadata": {},
   "source": [
    "# Lab 1 — Tiny VLM Adversarial Cost Challenge\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to the ML Security Lab! In this lab, you'll implement adversarial attacks against a Vision-Language Model (VLM) using the custom dataset and a frozen TinyCLIP scorer.\n",
    "\n",
    "### Objective\n",
    "Your task is to build an attack function that can manipulate either:\n",
    "- **Caption tokens** (text modifications)\n",
    "- **Image pixels** (visual modifications)\n",
    "- **Both** (multimodal attack)\n",
    "\n",
    "The goal is to flip the model's decision (match → no-match or vice versa) while minimizing the attack cost.\n",
    "\n",
    "### Constraints\n",
    "- **T_MAX = 10**: Maximum token edits per sample\n",
    "- **P_MAX = 100**: Maximum pixel edits per sample  \n",
    "- **Q_MAX = 100**: Maximum queries per sample\n",
    "- **Evaluation**: Public leaderboard (1,000 val pairs) + Private leaderboard (1,000 test pairs)\n",
    "\n",
    "### Scoring\n",
    "Your attack will be evaluated based on:\n",
    "1. **Success Rate**: Percentage of samples where you successfully flip the decision\n",
    "2. **Cost Efficiency**: Lower total cost (token edits + pixel edits + queries) is better\n",
    "3. **Attack Budget**: Must stay within the specified limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5244d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Check if 'images' directory or 'val_pairs.json' file is missing\n",
    "if not os.path.exists('images') or not os.path.exists('val_pairs.json'):\n",
    "    print(\"Required data not found. Extracting 'data.zip'...\")\n",
    "    \n",
    "    # Check if 'data.zip' exists\n",
    "    if os.path.exists('data.zip'):\n",
    "        with zipfile.ZipFile('data.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall()  # Extract all files in the current directory\n",
    "        print(\"Extraction complete!\")\n",
    "    else:\n",
    "        print(\"Error: 'data.zip' not found. Please ensure the file is in the current directory.\")\n",
    "else:\n",
    "    print(\"All required data is already present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab690789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import open_clip\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"All packages imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436ae18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "print(\"Loading validation data...\")\n",
    "\n",
    "# Load validation pairs from JSON\n",
    "with open('val_pairs.json', 'r') as f:\n",
    "    val_pairs = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(val_pairs)} validation pairs\")\n",
    "\n",
    "# Helper function to load images\n",
    "def load_image_from_pair(pair: dict) -> Image.Image:\n",
    "    \"\"\"Load image from the pair dictionary using image_path\"\"\"\n",
    "    return Image.open(pair['image_path']).convert('RGB')\n",
    "\n",
    "# Sample a few pairs to verify data loading\n",
    "print(\"\\nSample validation pairs:\")\n",
    "for i in range(3):\n",
    "    pair = val_pairs[i]\n",
    "    print(f\"  Image ID: {pair['image_id']}, Path: {pair['image_path']}\")\n",
    "    print(f\"  Caption: {pair['caption'][:50]}..., Match: {pair['is_match']}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Data distribution:\")\n",
    "labels = [pair['is_match'] for pair in val_pairs]\n",
    "print(f\"  Match (True): {sum(labels)}\")\n",
    "print(f\"  No-match (False): {len(labels) - sum(labels)}\")\n",
    "print(f\"  Balance: {sum(labels)/len(labels):.2%} positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f88fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TinyCLIP Scorer Implementation\n",
    "print(\"Loading CLIP model...\")\n",
    "\n",
    "# Try to load TinyCLIP, fallback to OpenCLIP ViT-B/32 if failed\n",
    "try:\n",
    "    # Attempt to load TinyCLIP from HuggingFace hub\n",
    "    model, preprocess, tokenizer = open_clip.create_model_and_transforms(\n",
    "        \"hf-hub:microsoft/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M\"\n",
    "    )\n",
    "    print(\"Successfully loaded TinyCLIP model\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load TinyCLIP: {e}\")\n",
    "    print(\"Falling back to OpenCLIP ViT-B/32...\")\n",
    "\n",
    "    model, preprocess, tokenizer = open_clip.create_model_and_transforms(\n",
    "        \"ViT-B-32\", \n",
    "        pretrained=\"laion2b_s34b_b79k\"\n",
    "    )\n",
    "    print(\"Successfully loaded OpenCLIP ViT-B/32\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3728b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_embed(image: Image.Image, caption: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between image and text embeddings.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        caption: Text string\n",
    "    \n",
    "    Returns:\n",
    "        Cosine similarity score between normalized embeddings\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Preprocess image\n",
    "        image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Tokenize text properly using open_clip tokenizer\n",
    "        text_tokens = open_clip.tokenize([caption]).to(device)\n",
    "        \n",
    "        # Get embeddings\n",
    "        image_features = model.encode_image(image_tensor)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = (image_features @ text_features.T).item()\n",
    "        \n",
    "    return similarity\n",
    "\n",
    "# Test the embedding function\n",
    "print(\"\\nTesting CLIP embedding function...\")\n",
    "test_pair = val_pairs[0]\n",
    "test_image = load_image_from_pair(test_pair)\n",
    "test_similarity = clip_embed(test_image, test_pair['caption'])\n",
    "print(f\"Sample similarity score: {test_similarity:.4f} (Expected match: {test_pair['is_match']})\")\n",
    "\n",
    "# Test on a few more samples\n",
    "print(\"\\nTesting on more samples:\")\n",
    "for i in range(3):\n",
    "    pair = val_pairs[i]\n",
    "    image = load_image_from_pair(pair)\n",
    "    similarity = clip_embed(image, pair['caption'])\n",
    "    print(f\"Sample {i+1}: similarity={similarity:.4f}, match={pair['is_match']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d850a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration: Fit logistic regression to get alpha, beta parameters\n",
    "print(\"Calibrating scorer with logistic regression...\")\n",
    "\n",
    "# Use first 200 samples for calibration\n",
    "tune_slice = val_pairs[:200]\n",
    "print(f\"Using {len(tune_slice)} samples for calibration\")\n",
    "\n",
    "# Compute similarities for calibration\n",
    "similarities = []\n",
    "ground_truths = []\n",
    "\n",
    "print(\"Computing similarities for calibration...\")\n",
    "for pair in tqdm(tune_slice, desc=\"Calibration\"):\n",
    "    image = load_image_from_pair(pair)\n",
    "    similarity = clip_embed(image, pair['caption'])\n",
    "    similarities.append(similarity)\n",
    "    ground_truths.append(int(pair['is_match']))\n",
    "\n",
    "similarities = np.array(similarities).reshape(-1, 1)\n",
    "ground_truths = np.array(ground_truths)\n",
    "\n",
    "# Fit logistic regression: sigmoid(alpha * cosine + beta)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(similarities, ground_truths)\n",
    "\n",
    "# Extract alpha and beta\n",
    "alpha = lr.coef_[0][0]  # Coefficient for similarity\n",
    "beta = lr.intercept_[0]  # Intercept\n",
    "\n",
    "print(f\"Calibration complete!\")\n",
    "print(f\"   Alpha (slope): {alpha:.4f}\")\n",
    "print(f\"   Beta (intercept): {beta:.4f}\")\n",
    "\n",
    "# Test calibration\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "test_similarities = similarities[:5].flatten()\n",
    "test_labels = ground_truths[:5]\n",
    "calibrated_probs = sigmoid(alpha * test_similarities + beta)\n",
    "\n",
    "print(f\"\\nCalibration test:\")\n",
    "for i in range(5):\n",
    "    print(f\"  Sim: {test_similarities[i]:.4f} → Prob: {calibrated_probs[i]:.4f}, True: {test_labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44228c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import label\n",
    "\n",
    "# BlackBox API Implementation\n",
    "import editdistance  # For Levenshtein distance\n",
    "\n",
    "class BlackBoxAPI:\n",
    "    \"\"\"\n",
    "    Black-box API for the VLM scorer with query budget tracking and cost calculation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float, beta: float, q_max: int = 200):\n",
    "        \"\"\"\n",
    "        Initialize the black-box API.\n",
    "        \n",
    "        Args:\n",
    "            alpha: Logistic regression slope parameter\n",
    "            beta: Logistic regression intercept parameter  \n",
    "            q_max: Maximum queries allowed per sample\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.q_max = q_max\n",
    "        self.query_count = 0\n",
    "        \n",
    "    def score(self, image_uint8: np.ndarray, caption_str: str) -> float:\n",
    "        \"\"\"\n",
    "        Score image-caption pair and return probability.\n",
    "        \n",
    "        Args:\n",
    "            image_uint8: Image as uint8 numpy array (H, W, C)\n",
    "            caption_str: Caption string\n",
    "            \n",
    "        Returns:\n",
    "            Probability in [0, 1] using sigmoid(alpha * cosine + beta)\n",
    "        \"\"\"\n",
    "        if self.query_count >= self.q_max:\n",
    "            raise RuntimeError(f\"Query budget exceeded! Used {self.query_count}/{self.q_max}\")\n",
    "            \n",
    "        # Convert numpy array to PIL Image\n",
    "        image_pil = Image.fromarray(image_uint8)\n",
    "        \n",
    "        # Get cosine similarity\n",
    "        cosine_sim = clip_embed(image_pil, caption_str)\n",
    "        \n",
    "        # Apply calibrated sigmoid\n",
    "        logit = self.alpha * cosine_sim + self.beta\n",
    "        probability = 1 / (1 + np.exp(-logit))\n",
    "        \n",
    "        self.query_count += 1\n",
    "        \n",
    "        return probability\n",
    "    \n",
    "    def reset_query_count(self):\n",
    "        \"\"\"Reset query counter for new sample.\"\"\"\n",
    "        self.query_count = 0\n",
    "        \n",
    "    def get_remaining_queries(self) -> int:\n",
    "        \"\"\"Get remaining query budget.\"\"\"\n",
    "        return self.q_max - self.query_count\n",
    "\n",
    "# Cost Functions\n",
    "def token_edit_cost(original: str, modified: str) -> int:\n",
    "    \"\"\"\n",
    "    Compute token-level Levenshtein distance using CLIP tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        original: Original caption\n",
    "        modified: Modified caption\n",
    "        \n",
    "    Returns:\n",
    "        Number of token edits (insertions, deletions, substitutions)\n",
    "    \"\"\"\n",
    "    # Use CLIP tokenizer for more accurate tokenization\n",
    "    orig_tokens = open_clip.tokenize([original], context_length=77)[0].numpy()\n",
    "    mod_tokens = open_clip.tokenize([modified], context_length=77)[0].numpy()\n",
    "    \n",
    "    # Remove padding tokens (0s) and special tokens for fair comparison\n",
    "    # Keep only actual content tokens\n",
    "    orig_tokens = orig_tokens[orig_tokens != 0]\n",
    "    mod_tokens = mod_tokens[mod_tokens != 0]\n",
    "    \n",
    "    return editdistance.eval(orig_tokens.tolist(), mod_tokens.tolist())\n",
    "\n",
    "def pixel_edit_cost(original: np.ndarray, modified: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Compute number of changed pixels with reduced cost for continuous regions.\n",
    "    \n",
    "    Args:\n",
    "        original: Original image as uint8 numpy array\n",
    "        modified: Modified image as uint8 numpy array\n",
    "        \n",
    "    Returns:\n",
    "        Adjusted cost based on number of changed pixels, with reduced cost for continuous regions.\n",
    "    \"\"\"\n",
    "    # Find the difference mask\n",
    "    diff_mask = np.any(original != modified, axis=-1)\n",
    "    \n",
    "    # Label connected components in the difference mask\n",
    "    labeled_regions, num_features = label(diff_mask)\n",
    "    \n",
    "    # Count pixels in each connected region\n",
    "    total_cost = 0\n",
    "    for region_id in range(1, num_features + 1):\n",
    "        region_size = np.sum(labeled_regions == region_id)\n",
    "        if region_size > 0:\n",
    "            # Full cost for the first pixel, half cost for the rest\n",
    "            total_cost += 1 + (region_size - 1) * 0.5\n",
    "    \n",
    "    return int(total_cost)\n",
    "\n",
    "# Test the BlackBox API\n",
    "print(\"Testing BlackBox API...\")\n",
    "\n",
    "# Initialize API with calibrated parameters\n",
    "api = BlackBoxAPI(alpha, beta, q_max=200)\n",
    "\n",
    "# Test on a sample\n",
    "test_pair = val_pairs[0]\n",
    "test_image = load_image_from_pair(test_pair)\n",
    "test_image_uint8 = np.array(test_image)\n",
    "\n",
    "# Get score\n",
    "score = api.score(test_image_uint8, test_pair['caption'])\n",
    "print(f\"API Score: {score:.4f} (Expected match: {test_pair['is_match']})\")\n",
    "print(f\"Queries used: {api.query_count}/{api.q_max}\")\n",
    "\n",
    "# Test cost functions\n",
    "original_caption = \"A cat sitting on a mat\"\n",
    "modified_caption = \"A dog standing on a rug\" \n",
    "token_cost = token_edit_cost(original_caption, modified_caption)\n",
    "print(f\"\\nToken edit cost example:\")\n",
    "print(f\"  Original: '{original_caption}'\")  \n",
    "print(f\"  Modified: '{modified_caption}'\")\n",
    "print(f\"  Cost: {token_cost} token edits\")\n",
    "\n",
    "# Test pixel cost (create a simple modification)\n",
    "original_img = np.zeros((100, 100, 3), dtype=np.uint8)\n",
    "modified_img = original_img.copy()\n",
    "modified_img[10:20, 10:20] = 255  # Change a 10x10 region\n",
    "pixel_cost = pixel_edit_cost(original_img, modified_img)\n",
    "print(f\"\\nPixel edit cost example:\")\n",
    "print(f\"  Modified {pixel_cost} pixels in 100x100 image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26049a39",
   "metadata": {},
   "source": [
    "## Your Task: Implement Adversarial Attacks\n",
    "\n",
    "### Attack Function Template\n",
    "\n",
    "Replace the trivial baseline in the `attack()` function with sophisticated adversarial attacks:\n",
    "\n",
    "```python\n",
    "def attack(image_np_uint8, caption_str, api, budgets):\n",
    "    # Your attack implementation here!\n",
    "    # You can modify:\n",
    "    # - Caption tokens (text modifications)  \n",
    "    # - Image pixels (visual modifications)\n",
    "    # - Both (multimodal attack)\n",
    "    \n",
    "    # Stay within budgets:\n",
    "    # - budgets['T_MAX'] = 10  token edits\n",
    "    # - budgets['P_MAX'] = 100 pixel edits\n",
    "    # - budgets['Q_MAX'] = 100 queries\n",
    "    \n",
    "    return {\n",
    "        'success': success,      # bool: did you flip the decision?\n",
    "        'image': final_image,    # np.array: attacked image\n",
    "        'caption': final_caption, # str: attacked caption  \n",
    "        'token_cost': token_cost, # int: tokens changed\n",
    "        'pixel_cost': pixel_cost, # int: pixels changed\n",
    "        'query_cost': query_cost  # int: API calls made\n",
    "    }\n",
    "```\n",
    "\n",
    "### Attack Strategies to Consider\n",
    "\n",
    "- **Text Attacks**: Synonym replacement, word insertion/deletion, semantic paraphrasing\n",
    "- **Image Attacks**: Adversarial noise, targeted pixel modifications, patch attacks\n",
    "- **Query Optimization**: Gradient-free optimization, genetic algorithms, hill climbing\n",
    "- **Multimodal**: Combined text+image attacks for maximum effectiveness\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "Your attack will be scored as: **ASR - 0.5×ANC - 0.1×(AQ/Q_MAX)**\n",
    "\n",
    "- **ASR**: Attack Success Rate (higher is better)\n",
    "- **ANC**: Average Number of Changes (lower is better) \n",
    "- **AQ**: Average Queries (lower is better)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Implement your attack** in the `attack()` function above\n",
    "2. **Test locally** using the evaluation framework  \n",
    "3. **Run on full dataset** by changing `max_samples=None`\n",
    "\n",
    "Good luck! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb55729",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">**FILL THIS CODE BLOCK**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165ef22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(image_np_uint8: np.ndarray, caption_str: str, api: BlackBoxAPI, budgets: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Student attack function to implement adversarial attacks.\n",
    "    \n",
    "    Args:\n",
    "        image_np_uint8: Original image as uint8 numpy array (H, W, C)\n",
    "        caption_str: Original caption string\n",
    "        api: BlackBoxAPI instance for querying the model\n",
    "        budgets: Dictionary with 'T_MAX', 'P_MAX', 'Q_MAX' limits\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - 'success': bool, whether attack succeeded (flipped decision)\n",
    "        - 'image': np.ndarray, final attacked image  \n",
    "        - 'caption': str, final attacked caption\n",
    "        - 'token_cost': int, number of token edits used\n",
    "        - 'pixel_cost': int, number of pixel edits used\n",
    "        - 'query_cost': int, number of queries used\n",
    "    \"\"\"\n",
    "    \n",
    "    # TRIVIAL BASELINE - Students should replace this!\n",
    "    # This baseline just returns the original inputs without any attack\n",
    "    \n",
    "    original_image = image_np_uint8.copy()\n",
    "    original_caption = caption_str\n",
    "    \n",
    "    # Get original score to determine target (flip the decision)\n",
    "    original_score = api.score(original_image, original_caption)\n",
    "    \n",
    "\n",
    "    # For this baseline, we don't actually perform any attack\n",
    "    # Students should implement sophisticated attacks here!\n",
    "    \n",
    "    # TODO: IMPLEMENT YOUR ATTACK HERE!\n",
    "    # You can modify:\n",
    "    # - Caption tokens (text modifications)  \n",
    "    # - Image pixels (visual modifications)\n",
    "    # - Both (multimodal attack)\n",
    "    \n",
    "    # Stay within budgets:\n",
    "    # - budgets['T_MAX'] = maximum token edits\n",
    "    # - budgets['P_MAX'] = maximum pixel edits\n",
    "    # - budgets['Q_MAX'] = maximum queries\n",
    "    \n",
    "    # Attack Strategies to Consider:\n",
    "    # - Text Attacks: Synonym replacement, word insertion/deletion, semantic paraphrasing\n",
    "    # - Image Attacks: Adversarial noise, targeted pixel modifications, patch attacks\n",
    "    # - Query Optimization: Gradient-free optimization, genetic algorithms, hill climbing\n",
    "    # - Multimodal: Combined text+image attacks for maximum effectiveness\n",
    "\n",
    "    final_image = original_image        # Comment out this line when implementing image attacks\n",
    "    final_caption = original_caption    # Comment out this line when implementing text attacks\n",
    "    \n",
    "    # Get final score  \n",
    "    final_score = api.score(final_image, final_caption)\n",
    "    \n",
    "    # Check if attack succeeded (decision flipped)\n",
    "    original_decision = original_score > 0.5\n",
    "    final_decision = final_score > 0.5\n",
    "    success = (original_decision != final_decision)\n",
    "    \n",
    "    # Calculate costs\n",
    "    token_cost = token_edit_cost(original_caption, final_caption)\n",
    "    pixel_cost = pixel_edit_cost(original_image, final_image)\n",
    "    query_cost = api.query_count\n",
    "    \n",
    "    return {\n",
    "        'success': success,\n",
    "        'image': final_image,\n",
    "        'caption': final_caption,\n",
    "        'token_cost': token_cost,\n",
    "        'pixel_cost': pixel_cost,  \n",
    "        'query_cost': query_cost,\n",
    "        'original_score': original_score,\n",
    "        'final_score': final_score\n",
    "    }\n",
    "\n",
    "print(\"Attack function defined (TRIVIAL BASELINE)\")\n",
    "print(\"   Students should replace the trivial implementation with sophisticated attacks!\")\n",
    "print(\"   Current baseline: Returns original inputs unchanged (0% success rate expected)\")\n",
    "\n",
    "# Test the attack function\n",
    "print(\"\\nTesting attack function...\")\n",
    "test_pair = val_pairs[0]\n",
    "test_image = np.array(load_image_from_pair(test_pair))\n",
    "\n",
    "# Create fresh API instance  \n",
    "test_api = BlackBoxAPI(alpha, beta, q_max=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a10633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Framework\n",
    "def evaluate_attack(val_pairs: list, attack_function, alpha: float, beta: float, budgets: dict, max_samples: int = None):\n",
    "    \"\"\"\n",
    "    Evaluate attack function on validation pairs.\n",
    "    \n",
    "    Args:\n",
    "        val_pairs: List of validation pairs\n",
    "        attack_function: Attack function to evaluate\n",
    "        alpha, beta: Calibrated parameters\n",
    "        budgets: Attack budgets dictionary\n",
    "        max_samples: Limit number of samples (None = all)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting evaluation...\")\n",
    "    \n",
    "    # Limit samples if specified\n",
    "    eval_pairs = val_pairs[:max_samples] if max_samples else val_pairs\n",
    "    print(f\"Evaluating on {len(eval_pairs)} samples\")\n",
    "    \n",
    "    results = []\n",
    "    total_success = 0\n",
    "    total_token_cost = 0\n",
    "    total_pixel_cost = 0\n",
    "    total_query_cost = 0\n",
    "    \n",
    "    for i, pair in enumerate(tqdm(eval_pairs, desc=\"Attacking\")):\n",
    "        # Create fresh API instance for each sample\n",
    "        api = BlackBoxAPI(alpha, beta, q_max=budgets['Q_MAX'])\n",
    "        \n",
    "        # Load image\n",
    "        image = np.array(load_image_from_pair(pair))\n",
    "        caption = pair['caption']\n",
    "        \n",
    "        try:\n",
    "            # Run attack\n",
    "            result = attack_function(image, caption, api, budgets)\n",
    "            \n",
    "            # Validate budget constraints\n",
    "            budget_valid = (\n",
    "                result['token_cost'] <= budgets['T_MAX'] and\n",
    "                result['pixel_cost'] <= budgets['P_MAX'] and  \n",
    "                result['query_cost'] <= budgets['Q_MAX']\n",
    "            )\n",
    "            \n",
    "            if not budget_valid:\n",
    "                print(f\"Sample {i}: Budget violation!\")\n",
    "                print(f\"   Tokens: {result['token_cost']}/{budgets['T_MAX']}\")\n",
    "                print(f\"   Pixels: {result['pixel_cost']}/{budgets['P_MAX']}\")  \n",
    "                print(f\"   Queries: {result['query_cost']}/{budgets['Q_MAX']}\")\n",
    "                result['success'] = False  # Invalid attacks count as failures\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            if result['success']:\n",
    "                total_success += 1\n",
    "            total_token_cost += result['token_cost']\n",
    "            total_pixel_cost += result['pixel_cost']\n",
    "            total_query_cost += result['query_cost']\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Sample {i}: Attack failed with error: {e}\")\n",
    "            # Add failed result\n",
    "            results.append({\n",
    "                'success': False,\n",
    "                'token_cost': budgets['T_MAX'],  # Penalize failures\n",
    "                'pixel_cost': budgets['P_MAX'], \n",
    "                'query_cost': budgets['Q_MAX'],\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    n_samples = len(results)\n",
    "    asr = total_success / n_samples  # Attack Success Rate\n",
    "    anc = (10*total_token_cost + total_pixel_cost) / n_samples  # Average Number of Changes  \n",
    "    aq = total_query_cost / n_samples  # Average Queries\n",
    "    \n",
    "    # Final score: ASR - 0.5*ANC - 0.1*(AQ/Q_MAX)\n",
    "    score = asr - 0.5 * (anc / (10*budgets['T_MAX'] + budgets['P_MAX'])) - 0.1 * (aq / budgets['Q_MAX'])\n",
    "    \n",
    "    evaluation_result = {\n",
    "        'ASR': asr,\n",
    "        'ANC': anc, \n",
    "        'AQ': aq,\n",
    "        'Score': score,\n",
    "        'n_samples': n_samples,\n",
    "        'total_success': total_success,\n",
    "        'avg_token_cost': total_token_cost / n_samples,\n",
    "        'avg_pixel_cost': total_pixel_cost / n_samples,\n",
    "        'budgets': budgets,\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    return evaluation_result\n",
    "\n",
    "# Run Evaluation\n",
    "print(\"Running evaluation on validation set...\")\n",
    "\n",
    "# Define attack budgets\n",
    "attack_budgets = {\n",
    "    'T_MAX': 10,     # Maximum token edits per sample\n",
    "    'P_MAX': 100,   # Maximum pixel edits per sample  \n",
    "    'Q_MAX': 100    # Maximum queries per sample\n",
    "}\n",
    "\n",
    "# Evaluate on subset first (faster for testing)\n",
    "print(\"Running on first 50 samples for quick testing...\")\n",
    "eval_result = evaluate_attack(\n",
    "    val_pairs=val_pairs, \n",
    "    attack_function=attack,\n",
    "    alpha=alpha,\n",
    "    beta=beta, \n",
    "    budgets=attack_budgets,\n",
    "    max_samples=50  # Quick test on 50 samples\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nEVALUATION RESULTS (50 samples):\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Attack Success Rate (ASR): {eval_result['ASR']:.1%}\")\n",
    "print(f\"Average Number of Changes (ANC): {eval_result['ANC']:.2f}\")  \n",
    "print(f\"Average Queries (AQ): {eval_result['AQ']:.1f}\")\n",
    "print(f\"Final Score: {eval_result['Score']:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Budget Usage:\")\n",
    "print(f\"  Avg Token Cost: {eval_result['avg_token_cost']:.2f}/{attack_budgets['T_MAX']}\")\n",
    "print(f\"  Avg Pixel Cost: {eval_result['avg_pixel_cost']:.2f}/{attack_budgets['P_MAX']}\")  \n",
    "print(f\"  Avg Query Cost: {eval_result['AQ']:.1f}/{attack_budgets['Q_MAX']}\")\n",
    "print(f\"\\nNOTE: This is a trivial baseline (0% ASR expected)\")\n",
    "print(f\"Students should implement sophisticated attacks to improve ASR!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a906503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Evaluation (Uncomment when ready to test your attack)\n",
    "\n",
    "def run_full_evaluation():\n",
    "    \"\"\"Run evaluation on all 1000 validation samples.\"\"\"\n",
    "    print(\"Running FULL evaluation on all 1000 validation samples...\")\n",
    "    print(\"This may take several minutes depending on your attack implementation.\")\n",
    "    \n",
    "    full_result = evaluate_attack(\n",
    "        val_pairs=val_pairs,\n",
    "        attack_function=attack, \n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        budgets=attack_budgets,\n",
    "        max_samples=None  # All samples\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFINAL EVALUATION RESULTS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Attack Success Rate (ASR): {full_result['ASR']:.1%}\")\n",
    "    print(f\"Average Number of Changes (ANC): {full_result['ANC']:.2f}\")\n",
    "    print(f\"Average Queries (AQ): {full_result['AQ']:.1f}\")\n",
    "    print(f\"Final Score: {full_result['Score']:.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return full_result\n",
    "\n",
    "# Uncomment the line below when ready to run full evaluation:\n",
    "full_results = run_full_evaluation()\n",
    "\n",
    "print(\"To run full evaluation on all 1000 samples:\")\n",
    "print(\"Uncomment: full_results = run_full_evaluation()\")\n",
    "print(\"\\nCurrent status: Framework ready for student implementations!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
