{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0666fbb",
   "metadata": {},
   "source": [
    "# Lab 1 — Tiny VLM Adversarial Cost Challenge\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to the ML Security Lab! In this lab, you'll implement adversarial attacks against a Vision-Language Model (VLM) using the custom dataset and a frozen TinyCLIP scorer.\n",
    "\n",
    "### Objective\n",
    "Your task is to build an attack function that can manipulate either:\n",
    "- **Caption tokens** (text modifications)\n",
    "- **Image pixels** (visual modifications)\n",
    "- **Both** (multimodal attack)\n",
    "\n",
    "The goal is to flip the model's decision (match → no-match or vice versa) while minimizing the attack cost.\n",
    "\n",
    "### Constraints\n",
    "- **T_MAX = 10**: Maximum token edits per sample\n",
    "- **P_MAX = 100**: Maximum pixel edits per sample  \n",
    "- **Q_MAX = 100**: Maximum queries per sample\n",
    "- **Evaluation**: Public leaderboard (1,000 val pairs) + Private leaderboard (1,000 test pairs)\n",
    "\n",
    "### Scoring\n",
    "Your attack will be evaluated based on:\n",
    "1. **Success Rate**: Percentage of samples where you successfully flip the decision\n",
    "2. **Cost Efficiency**: Lower total cost (token edits + pixel edits + queries) is better\n",
    "3. **Attack Budget**: Must stay within the specified limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5244d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required data is already present.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Check if 'images' directory or 'val_pairs.json' file is missing\n",
    "if not os.path.exists('images') or not os.path.exists('val_pairs.json'):\n",
    "    print(\"Required data not found. Extracting 'data.zip'...\")\n",
    "    \n",
    "    # Check if 'data.zip' exists\n",
    "    if os.path.exists('data.zip'):\n",
    "        with zipfile.ZipFile('data.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall()  # Extract all files in the current directory\n",
    "        print(\"Extraction complete!\")\n",
    "    else:\n",
    "        print(\"Error: 'data.zip' not found. Please ensure the file is in the current directory.\")\n",
    "else:\n",
    "    print(\"All required data is already present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab690789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages imported successfully!\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import open_clip\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"All packages imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "436ae18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data...\n",
      "Loaded 1000 validation pairs\n",
      "\n",
      "Sample validation pairs:\n",
      "  Image ID: 38137, Path: images/val/000000119402.jpg\n",
      "  Caption: A gold bus traveling on a single lane road..., Match: True\n",
      "\n",
      "  Image ID: 15194, Path: images/val/000000404780.jpg\n",
      "  Caption: Two women in the snow on skis in front of a large ..., Match: False\n",
      "\n",
      "  Image ID: 19082, Path: images/val/000000148898.jpg\n",
      "  Caption: A man jumping a brown horse over an obstacle...., Match: False\n",
      "\n",
      "Data distribution:\n",
      "  Match (True): 500\n",
      "  No-match (False): 500\n",
      "  Balance: 50.00% positive\n"
     ]
    }
   ],
   "source": [
    "# Data Loading\n",
    "print(\"Loading validation data...\")\n",
    "\n",
    "# Load validation pairs from JSON\n",
    "with open('val_pairs.json', 'r') as f:\n",
    "    val_pairs = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(val_pairs)} validation pairs\")\n",
    "\n",
    "# Helper function to load images\n",
    "def load_image_from_pair(pair: dict) -> Image.Image:\n",
    "    \"\"\"Load image from the pair dictionary using image_path\"\"\"\n",
    "    return Image.open(pair['image_path']).convert('RGB')\n",
    "\n",
    "# Sample a few pairs to verify data loading\n",
    "print(\"\\nSample validation pairs:\")\n",
    "for i in range(3):\n",
    "    pair = val_pairs[i]\n",
    "    print(f\"  Image ID: {pair['image_id']}, Path: {pair['image_path']}\")\n",
    "    print(f\"  Caption: {pair['caption'][:50]}..., Match: {pair['is_match']}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Data distribution:\")\n",
    "labels = [pair['is_match'] for pair in val_pairs]\n",
    "print(f\"  Match (True): {sum(labels)}\")\n",
    "print(f\"  No-match (False): {len(labels) - sum(labels)}\")\n",
    "print(f\"  Balance: {sum(labels)/len(labels):.2%} positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f88fe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model...\n",
      "Failed to load TinyCLIP: Failed initial config/weights load from HF Hub microsoft/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M: Failed to download file (open_clip_config.json) for microsoft/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M. Last error: 404 Client Error. (Request ID: Root=1-68e0baa7-1dc50171703be15951316f1c;819ab7e2-e670-4da3-9e49-09736cf8d2cc)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/microsoft/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M/resolve/main/open_clip_config.json.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "Falling back to OpenCLIP ViT-B/32...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded OpenCLIP ViT-B/32\n",
      "Model loaded on: cuda\n",
      "<class 'torchvision.transforms.transforms.Compose'>\n",
      "Compose(\n",
      "    RandomResizedCrop(size=(224, 224), scale=(0.9, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)\n",
      "    <function _convert_to_rgb at 0x7fed8fc6e0c0>\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# TinyCLIP Scorer Implementation\n",
    "print(\"Loading CLIP model...\")\n",
    "\n",
    "# Try to load TinyCLIP, fallback to OpenCLIP ViT-B/32 if failed\n",
    "try:\n",
    "    # Attempt to load TinyCLIP from HuggingFace hub\n",
    "    model, preprocess, tokenizer = open_clip.create_model_and_transforms(\n",
    "        \"hf-hub:microsoft/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M\"\n",
    "    )\n",
    "    print(\"Successfully loaded TinyCLIP model\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load TinyCLIP: {e}\")\n",
    "    print(\"Falling back to OpenCLIP ViT-B/32...\")\n",
    "\n",
    "    model, preprocess, tokenizer = open_clip.create_model_and_transforms(\n",
    "        \"ViT-B-32\", \n",
    "        pretrained=\"laion2b_s34b_b79k\"\n",
    "    )\n",
    "    print(\"Successfully loaded OpenCLIP ViT-B/32\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3728b433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing CLIP embedding function...\n",
      "Sample similarity score: 0.3473 (Expected match: True)\n",
      "\n",
      "Testing on more samples:\n",
      "Sample 1: similarity=0.3543, match=True\n",
      "Sample 2: similarity=0.0343, match=False\n",
      "Sample 3: similarity=0.0779, match=False\n"
     ]
    }
   ],
   "source": [
    "def clip_embed(image: Image.Image, caption: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between image and text embeddings.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        caption: Text string\n",
    "    \n",
    "    Returns:\n",
    "        Cosine similarity score between normalized embeddings\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Preprocess image\n",
    "        image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Tokenize text properly using open_clip tokenizer\n",
    "        text_tokens = open_clip.tokenize([caption]).to(device)\n",
    "        \n",
    "        # Get embeddings\n",
    "        image_features = model.encode_image(image_tensor)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = (image_features @ text_features.T).item()\n",
    "        \n",
    "    return similarity\n",
    "\n",
    "# Test the embedding function\n",
    "print(\"\\nTesting CLIP embedding function...\")\n",
    "test_pair = val_pairs[0]\n",
    "test_image = load_image_from_pair(test_pair)\n",
    "test_similarity = clip_embed(test_image, test_pair['caption'])\n",
    "print(f\"Sample similarity score: {test_similarity:.4f} (Expected match: {test_pair['is_match']})\")\n",
    "\n",
    "# Test on a few more samples\n",
    "print(\"\\nTesting on more samples:\")\n",
    "for i in range(3):\n",
    "    pair = val_pairs[i]\n",
    "    image = load_image_from_pair(pair)\n",
    "    similarity = clip_embed(image, pair['caption'])\n",
    "    print(f\"Sample {i+1}: similarity={similarity:.4f}, match={pair['is_match']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d850a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating scorer with logistic regression...\n",
      "Using 200 samples for calibration\n",
      "Computing similarities for calibration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibration: 100%|██████████| 200/200 [00:03<00:00, 64.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration complete!\n",
      "   Alpha (slope): 6.6427\n",
      "   Beta (intercept): -1.1133\n",
      "\n",
      "Calibration test:\n",
      "  Sim: 0.3779 → Prob: 0.8017, True: 1\n",
      "  Sim: 0.0343 → Prob: 0.2921, True: 0\n",
      "  Sim: 0.0812 → Prob: 0.3604, True: 0\n",
      "  Sim: 0.1899 → Prob: 0.5370, True: 0\n",
      "  Sim: 0.3772 → Prob: 0.8010, True: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calibration: Fit logistic regression to get alpha, beta parameters\n",
    "print(\"Calibrating scorer with logistic regression...\")\n",
    "\n",
    "# Use first 200 samples for calibration\n",
    "tune_slice = val_pairs[:200]\n",
    "print(f\"Using {len(tune_slice)} samples for calibration\")\n",
    "\n",
    "# Compute similarities for calibration\n",
    "similarities = []\n",
    "ground_truths = []\n",
    "\n",
    "print(\"Computing similarities for calibration...\")\n",
    "for pair in tqdm(tune_slice, desc=\"Calibration\"):\n",
    "    image = load_image_from_pair(pair)\n",
    "    similarity = clip_embed(image, pair['caption'])\n",
    "    similarities.append(similarity)\n",
    "    ground_truths.append(int(pair['is_match']))\n",
    "\n",
    "similarities = np.array(similarities).reshape(-1, 1)\n",
    "ground_truths = np.array(ground_truths)\n",
    "\n",
    "# Fit logistic regression: sigmoid(alpha * cosine + beta)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(similarities, ground_truths)\n",
    "\n",
    "# Extract alpha and beta\n",
    "alpha = lr.coef_[0][0]  # Coefficient for similarity\n",
    "beta = lr.intercept_[0]  # Intercept\n",
    "\n",
    "print(f\"Calibration complete!\")\n",
    "print(f\"   Alpha (slope): {alpha:.4f}\")\n",
    "print(f\"   Beta (intercept): {beta:.4f}\")\n",
    "\n",
    "# Test calibration\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "test_similarities = similarities[:5].flatten()\n",
    "test_labels = ground_truths[:5]\n",
    "calibrated_probs = sigmoid(alpha * test_similarities + beta)\n",
    "\n",
    "print(f\"\\nCalibration test:\")\n",
    "for i in range(5):\n",
    "    print(f\"  Sim: {test_similarities[i]:.4f} → Prob: {calibrated_probs[i]:.4f}, True: {test_labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723835c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 480, 640])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "only Tensors of floating point dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 46\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# this is how it should be done according to the preprocess function\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# but its kinda wrong since the preprocess function is maybe for training\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# idk maybe we can find it out later, so the gradient can be passed through it\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#image_tensor = preprocess(image).unsqueeze(0).to(device)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 46\u001b[0m image_grad \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_image_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcaption\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mis_match\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m clip_loss(image_tensor, pair[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m], pair[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_match\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     49\u001b[0m similarity \u001b[38;5;241m=\u001b[39m clip_embed(image, pair[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[80], line 33\u001b[0m, in \u001b[0;36mcalc_image_grad\u001b[0;34m(image_tensor, caption, label)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcalc_image_grad\u001b[39m(image_tensor, caption: \u001b[38;5;28mstr\u001b[39m, label) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mimage_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires_grad_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# allow gradient wrt input pixels\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     loss \u001b[38;5;241m=\u001b[39m clip_loss(image_tensor, caption, label)\n\u001b[1;32m     35\u001b[0m     image_grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(loss, [image_tensor])[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: only Tensors of floating point dtype can require gradients"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, ToTensor, Normalize, Compose\n",
    "\n",
    "def clip_loss(image_tensor, caption: str, label) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between image and text embeddings.\n",
    "    Returns a torch scalar that supports autograd.\n",
    "    \"\"\"\n",
    "    # Preprocess image (make sure to keep gradient tracking)\n",
    "\n",
    "    # Tokenize text (token embeddings are usually non-differentiable wrt raw string)\n",
    "    text_tokens = open_clip.tokenize([caption]).to(device)\n",
    "\n",
    "    # Get embeddings (these will remain differentiable wrt image_tensor)\n",
    "    image_features = model.encode_image(image_tensor)\n",
    "    text_features = model.encode_text(text_tokens)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    image_features = F.normalize(image_features, dim=-1)\n",
    "    text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "    similarity = (image_features @ text_features.T).squeeze()  # shape: scalar\n",
    "\n",
    "    output = similarity*alpha + beta\n",
    "    output = torch.sigmoid(output)\n",
    "    \n",
    "\n",
    "    # Cosine similarity\n",
    "    if label == 1:\n",
    "        loss = 1 - output  # want to maximize similarity\n",
    "    elif label == 0:\n",
    "        loss = output  # want to minimize similarity\n",
    "    return loss\n",
    "\n",
    "def calc_image_grad(image_tensor, caption: str, label) -> torch.Tensor:\n",
    "    image_tensor.requires_grad_(True)  # allow gradient wrt input pixels\n",
    "    loss = clip_loss(image_tensor, caption, label)\n",
    "    image_grad = torch.autograd.grad(loss, [image_tensor])[0]\n",
    "    return image_grad\n",
    "\n",
    "for pair in val_pairs[:5]:\n",
    "    image = load_image_from_pair(pair)\n",
    "    image_tensor = torch.tensor(np.array(image)).permute(2, 0, 1).unsqueeze(0).to(device)  # Change to (C, H, W)\n",
    "    # this is how it should be done according to the preprocess function\n",
    "    # but its kinda wrong since the preprocess function is maybe for training\n",
    "    # idk maybe we can find it out later, so the gradient can be passed through it\n",
    "    #image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "    print(image_tensor.shape)\n",
    "    image_grad = calc_image_grad(image_tensor, pair['caption'], pair['is_match'])\n",
    "    loss = clip_loss(image_tensor, pair['caption'], pair['is_match'])\n",
    "\n",
    "    similarity = clip_embed(image, pair['caption'])\n",
    "    print(f\"loss: {loss.item():.4f}, Similarity: {similarity:.4f}, Match: {pair['is_match']}\")\n",
    "    alpha = 10 # step size for adversarial perturbation\n",
    "    adversarial_image_tensor = torch.clamp(image_tensor + alpha * image_grad.sign(), 0, 255)\n",
    "    adversarial_image = PIL.Image.fromarray(adversarial_image_tensor.detach().squeeze().cpu().numpy().astype(np.uint8).transpose(1, 2, 0))\n",
    "    adversarial_loss = clip_loss(adversarial_image_tensor, pair['caption'], pair['is_match'])\n",
    "    adversarial_similarity = clip_embed(adversarial_image, pair['caption'])\n",
    "    print(f\"Adversarial loss: {adversarial_loss.item():.4f}, Adversarial Similarity: {adversarial_similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f3e2d5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.transforms.transforms.Compose'>\n",
      "Compose(\n",
      "    RandomResizedCrop(size=(224, 224), scale=(0.9, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic, antialias=True)\n",
      "    <function _convert_to_rgb at 0x7fed8fc6e0c0>\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#investigate the preprocessing:\n",
    "\n",
    "print(type(preprocess))\n",
    "print(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c8abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking up the code for clip _convert_to_rgb it is:\n",
    "def _convert_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "# aka the built-in RGB conversion from PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c44228c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing BlackBox API...\n",
      "API Score: 0.9191 (Expected match: True)\n",
      "Queries used: 1/200\n",
      "\n",
      "Token edit cost example:\n",
      "  Original: 'A cat sitting on a mat'\n",
      "  Modified: 'A dog standing on a rug'\n",
      "  Cost: 3 token edits\n",
      "\n",
      "Pixel edit cost example:\n",
      "  Modified 50 pixels in 100x100 image\n"
     ]
    }
   ],
   "source": [
    "from scipy.ndimage import label\n",
    "\n",
    "# BlackBox API Implementation\n",
    "import editdistance  # For Levenshtein distance\n",
    "\n",
    "class BlackBoxAPI:\n",
    "    \"\"\"\n",
    "    Black-box API for the VLM scorer with query budget tracking and cost calculation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float, beta: float, q_max: int = 200):\n",
    "        \"\"\"\n",
    "        Initialize the black-box API.\n",
    "        \n",
    "        Args:\n",
    "            alpha: Logistic regression slope parameter\n",
    "            beta: Logistic regression intercept parameter  \n",
    "            q_max: Maximum queries allowed per sample\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.q_max = q_max\n",
    "        self.query_count = 0\n",
    "        \n",
    "    def score(self, image_uint8: np.ndarray, caption_str: str) -> float:\n",
    "        \"\"\"\n",
    "        Score image-caption pair and return probability.\n",
    "        \n",
    "        Args:\n",
    "            image_uint8: Image as uint8 numpy array (H, W, C)\n",
    "            caption_str: Caption string\n",
    "            \n",
    "        Returns:\n",
    "            Probability in [0, 1] using sigmoid(alpha * cosine + beta)\n",
    "        \"\"\"\n",
    "        if self.query_count >= self.q_max:\n",
    "            raise RuntimeError(f\"Query budget exceeded! Used {self.query_count}/{self.q_max}\")\n",
    "            \n",
    "        # Convert numpy array to PIL Image\n",
    "        image_pil = Image.fromarray(image_uint8)\n",
    "        \n",
    "        # Get cosine similarity\n",
    "        cosine_sim = clip_embed(image_pil, caption_str)\n",
    "        \n",
    "        # Apply calibrated sigmoid\n",
    "        logit = self.alpha * cosine_sim + self.beta\n",
    "        probability = 1 / (1 + np.exp(-logit))\n",
    "        \n",
    "        self.query_count += 1\n",
    "        \n",
    "        return probability\n",
    "    \n",
    "    def reset_query_count(self):\n",
    "        \"\"\"Reset query counter for new sample.\"\"\"\n",
    "        self.query_count = 0\n",
    "        \n",
    "    def get_remaining_queries(self) -> int:\n",
    "        \"\"\"Get remaining query budget.\"\"\"\n",
    "        return self.q_max - self.query_count\n",
    "\n",
    "# Cost Functions\n",
    "def token_edit_cost(original: str, modified: str) -> int:\n",
    "    \"\"\"\n",
    "    Compute token-level Levenshtein distance using CLIP tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        original: Original caption\n",
    "        modified: Modified caption\n",
    "        \n",
    "    Returns:\n",
    "        Number of token edits (insertions, deletions, substitutions)\n",
    "    \"\"\"\n",
    "    # Use CLIP tokenizer for more accurate tokenization\n",
    "    orig_tokens = open_clip.tokenize([original], context_length=77)[0].numpy()\n",
    "    mod_tokens = open_clip.tokenize([modified], context_length=77)[0].numpy()\n",
    "    \n",
    "    # Remove padding tokens (0s) and special tokens for fair comparison\n",
    "    # Keep only actual content tokens\n",
    "    orig_tokens = orig_tokens[orig_tokens != 0]\n",
    "    mod_tokens = mod_tokens[mod_tokens != 0]\n",
    "    \n",
    "    return editdistance.eval(orig_tokens.tolist(), mod_tokens.tolist())\n",
    "\n",
    "def pixel_edit_cost(original: np.ndarray, modified: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Compute number of changed pixels with reduced cost for continuous regions.\n",
    "    \n",
    "    Args:\n",
    "        original: Original image as uint8 numpy array\n",
    "        modified: Modified image as uint8 numpy array\n",
    "        \n",
    "    Returns:\n",
    "        Adjusted cost based on number of changed pixels, with reduced cost for continuous regions.\n",
    "    \"\"\"\n",
    "    # Find the difference mask\n",
    "    diff_mask = np.any(original != modified, axis=-1)\n",
    "    \n",
    "    # Label connected components in the difference mask\n",
    "    labeled_regions, num_features = label(diff_mask)\n",
    "    \n",
    "    # Count pixels in each connected region\n",
    "    total_cost = 0\n",
    "    for region_id in range(1, num_features + 1):\n",
    "        region_size = np.sum(labeled_regions == region_id)\n",
    "        if region_size > 0:\n",
    "            # Full cost for the first pixel, half cost for the rest\n",
    "            total_cost += 1 + (region_size - 1) * 0.5\n",
    "    \n",
    "    return int(total_cost)\n",
    "\n",
    "# Test the BlackBox API\n",
    "print(\"Testing BlackBox API...\")\n",
    "\n",
    "# Initialize API with calibrated parameters\n",
    "api = BlackBoxAPI(alpha, beta, q_max=200)\n",
    "\n",
    "# Test on a sample\n",
    "test_pair = val_pairs[0]\n",
    "test_image = load_image_from_pair(test_pair)\n",
    "test_image_uint8 = np.array(test_image)\n",
    "\n",
    "# Get score\n",
    "score = api.score(test_image_uint8, test_pair['caption'])\n",
    "print(f\"API Score: {score:.4f} (Expected match: {test_pair['is_match']})\")\n",
    "print(f\"Queries used: {api.query_count}/{api.q_max}\")\n",
    "\n",
    "# Test cost functions\n",
    "original_caption = \"A cat sitting on a mat\"\n",
    "modified_caption = \"A dog standing on a rug\" \n",
    "token_cost = token_edit_cost(original_caption, modified_caption)\n",
    "print(f\"\\nToken edit cost example:\")\n",
    "print(f\"  Original: '{original_caption}'\")  \n",
    "print(f\"  Modified: '{modified_caption}'\")\n",
    "print(f\"  Cost: {token_cost} token edits\")\n",
    "\n",
    "# Test pixel cost (create a simple modification)\n",
    "original_img = np.zeros((100, 100, 3), dtype=np.uint8)\n",
    "modified_img = original_img.copy()\n",
    "modified_img[10:20, 10:20] = 255  # Change a 10x10 region\n",
    "pixel_cost = pixel_edit_cost(original_img, modified_img)\n",
    "print(f\"\\nPixel edit cost example:\")\n",
    "print(f\"  Modified {pixel_cost} pixels in 100x100 image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26049a39",
   "metadata": {},
   "source": [
    "## Your Task: Implement Adversarial Attacks\n",
    "\n",
    "### Attack Function Template\n",
    "\n",
    "Replace the trivial baseline in the `attack()` function with sophisticated adversarial attacks:\n",
    "\n",
    "```python\n",
    "def attack(image_np_uint8, caption_str, api, budgets):\n",
    "    # Your attack implementation here!\n",
    "    # You can modify:\n",
    "    # - Caption tokens (text modifications)  \n",
    "    # - Image pixels (visual modifications)\n",
    "    # - Both (multimodal attack)\n",
    "    \n",
    "    # Stay within budgets:\n",
    "    # - budgets['T_MAX'] = 10  token edits\n",
    "    # - budgets['P_MAX'] = 100 pixel edits\n",
    "    # - budgets['Q_MAX'] = 100 queries\n",
    "    \n",
    "    return {\n",
    "        'success': success,      # bool: did you flip the decision?\n",
    "        'image': final_image,    # np.array: attacked image\n",
    "        'caption': final_caption, # str: attacked caption  \n",
    "        'token_cost': token_cost, # int: tokens changed\n",
    "        'pixel_cost': pixel_cost, # int: pixels changed\n",
    "        'query_cost': query_cost  # int: API calls made\n",
    "    }\n",
    "```\n",
    "\n",
    "### Attack Strategies to Consider\n",
    "\n",
    "- **Text Attacks**: Synonym replacement, word insertion/deletion, semantic paraphrasing\n",
    "- **Image Attacks**: Adversarial noise, targeted pixel modifications, patch attacks\n",
    "- **Query Optimization**: Gradient-free optimization, genetic algorithms, hill climbing\n",
    "- **Multimodal**: Combined text+image attacks for maximum effectiveness\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "Your attack will be scored as: **ASR - 0.5×ANC - 0.1×(AQ/Q_MAX)**\n",
    "\n",
    "- **ASR**: Attack Success Rate (higher is better)\n",
    "- **ANC**: Average Number of Changes (lower is better) \n",
    "- **AQ**: Average Queries (lower is better)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Implement your attack** in the `attack()` function above\n",
    "2. **Test locally** using the evaluation framework  \n",
    "3. **Run on full dataset** by changing `max_samples=None`\n",
    "\n",
    "Good luck! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb55729",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">**FILL THIS CODE BLOCK**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "165ef22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack function defined (TRIVIAL BASELINE)\n",
      "   Students should replace the trivial implementation with sophisticated attacks!\n",
      "   Current baseline: Returns original inputs unchanged (0% success rate expected)\n",
      "\n",
      "Testing attack function...\n",
      "(480, 640, 3)\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "(224, 224, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (480,640,3) (224,224,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 105\u001b[0m\n\u001b[1;32m     97\u001b[0m test_api \u001b[38;5;241m=\u001b[39m BlackBoxAPI(alpha, beta, q_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     99\u001b[0m attack_budgets \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT_MAX\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m,     \u001b[38;5;66;03m# Maximum token edits per sample\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP_MAX\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m,   \u001b[38;5;66;03m# Maximum pixel edits per sample  \u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ_MAX\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m    \u001b[38;5;66;03m# Maximum queries per sample\u001b[39;00m\n\u001b[1;32m    103\u001b[0m }\n\u001b[0;32m--> 105\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcaption\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattack_budgets\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[73], line 73\u001b[0m, in \u001b[0;36mattack\u001b[0;34m(image_np_uint8, caption_str, api, budgets)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Calculate costs\u001b[39;00m\n\u001b[1;32m     72\u001b[0m token_cost \u001b[38;5;241m=\u001b[39m token_edit_cost(original_caption, final_caption)\n\u001b[0;32m---> 73\u001b[0m pixel_cost \u001b[38;5;241m=\u001b[39m \u001b[43mpixel_edit_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m query_cost \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mquery_count\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m'\u001b[39m: success,\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: final_image,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_score\u001b[39m\u001b[38;5;124m'\u001b[39m: final_score\n\u001b[1;32m     85\u001b[0m }\n",
      "Cell \u001b[0;32mIn[11], line 96\u001b[0m, in \u001b[0;36mpixel_edit_cost\u001b[0;34m(original, modified)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03mCompute number of changed pixels with reduced cost for continuous regions.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    Adjusted cost based on number of changed pixels, with reduced cost for continuous regions.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Find the difference mask\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m diff_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39many(\u001b[43moriginal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodified\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Label connected components in the difference mask\u001b[39;00m\n\u001b[1;32m     99\u001b[0m labeled_regions, num_features \u001b[38;5;241m=\u001b[39m label(diff_mask)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (480,640,3) (224,224,3) "
     ]
    }
   ],
   "source": [
    "def attack(image_np_uint8: np.ndarray, caption_str: str, api: BlackBoxAPI, budgets: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Student attack function to implement adversarial attacks.\n",
    "    \n",
    "    Args:\n",
    "        image_np_uint8: Original image as uint8 numpy array (H, W, C)\n",
    "        caption_str: Original caption string\n",
    "        api: BlackBoxAPI instance for querying the model\n",
    "        budgets: Dictionary with 'T_MAX', 'P_MAX', 'Q_MAX' limits\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - 'success': bool, whether attack succeeded (flipped decision)\n",
    "        - 'image': np.ndarray, final attacked image  \n",
    "        - 'caption': str, final attacked caption\n",
    "        - 'token_cost': int, number of token edits used\n",
    "        - 'pixel_cost': int, number of pixel edits used\n",
    "        - 'query_cost': int, number of queries used\n",
    "    \"\"\"\n",
    "    \n",
    "    # TRIVIAL BASELINE - Students should replace this!\n",
    "    # This baseline just returns the original inputs without any attack\n",
    "    \n",
    "    original_image = image_np_uint8.copy()\n",
    "    original_caption = caption_str\n",
    "    \n",
    "    # Get original score to determine target (flip the decision)\n",
    "    original_score = api.score(original_image, original_caption)\n",
    "    \n",
    "\n",
    "    # For this baseline, we don't actually perform any attack\n",
    "    # Students should implement sophisticated attacks here!\n",
    "    \n",
    "    # TODO: IMPLEMENT YOUR ATTACK HERE!\n",
    "    # You can modify:\n",
    "    # - Caption tokens (text modifications)  \n",
    "    # - Image pixels (visual modifications)\n",
    "    # - Both (multimodal attack)\n",
    "    \n",
    "    # Stay within budgets:\n",
    "    # - budgets['T_MAX'] = maximum token edits\n",
    "    # - budgets['P_MAX'] = maximum pixel edits\n",
    "    # - budgets['Q_MAX'] = maximum queries\n",
    "    \n",
    "    # Attack Strategies to Consider:\n",
    "    # - Text Attacks: Synonym replacement, word insertion/deletion, semantic paraphrasing\n",
    "    # - Image Attacks: Adversarial noise, targeted pixel modifications, patch attacks\n",
    "    # - Query Optimization: Gradient-free optimization, genetic algorithms, hill climbing\n",
    "    # - Multimodal: Combined text+image attacks for maximum effectiveness\n",
    "    print(original_image.shape)\n",
    "    image_tensor = preprocess(Image.fromarray(original_image)).unsqueeze(0).to(device)\n",
    "    print(image_tensor.shape)\n",
    "    image_grad = calc_image_grad(image_tensor, caption_str, 1)\n",
    "    print(image_grad.shape)\n",
    "    alpha = 10 # step size for adversarial perturbation\n",
    "    adversarial_image_tensor = torch.clamp(image_tensor + alpha * image_grad.sign(), 0, 255)\n",
    "    print(adversarial_image_tensor.shape)\n",
    "    adversarial_image = PIL.Image.fromarray(adversarial_image_tensor.detach().squeeze().cpu().numpy().astype(np.uint8).transpose(1, 2, 0))\n",
    "    final_image = np.array(adversarial_image)        # Comment out this line when implementing image attacks\n",
    "    print(final_image.shape)\n",
    "    final_caption = original_caption    # Comment out this line when implementing text attacks\n",
    "    \n",
    "    # Get final score  \n",
    "    final_score = api.score(final_image, final_caption)\n",
    "    \n",
    "    # Check if attack succeeded (decision flipped)\n",
    "    original_decision = original_score > 0.5\n",
    "    final_decision = final_score > 0.5\n",
    "    success = (original_decision != final_decision)\n",
    "    \n",
    "    # Calculate costs\n",
    "    token_cost = token_edit_cost(original_caption, final_caption)\n",
    "    pixel_cost = pixel_edit_cost(original_image, final_image)\n",
    "    query_cost = api.query_count\n",
    "    \n",
    "    return {\n",
    "        'success': success,\n",
    "        'image': final_image,\n",
    "        'caption': final_caption,\n",
    "        'token_cost': token_cost,\n",
    "        'pixel_cost': pixel_cost,  \n",
    "        'query_cost': query_cost,\n",
    "        'original_score': original_score,\n",
    "        'final_score': final_score\n",
    "    }\n",
    "\n",
    "print(\"Attack function defined (TRIVIAL BASELINE)\")\n",
    "print(\"   Students should replace the trivial implementation with sophisticated attacks!\")\n",
    "print(\"   Current baseline: Returns original inputs unchanged (0% success rate expected)\")\n",
    "\n",
    "# Test the attack function\n",
    "print(\"\\nTesting attack function...\")\n",
    "test_pair = val_pairs[0]\n",
    "test_image = np.array(load_image_from_pair(test_pair))\n",
    "\n",
    "# Create fresh API instance  \n",
    "test_api = BlackBoxAPI(alpha, beta, q_max=100)\n",
    "\n",
    "attack_budgets = {\n",
    "    'T_MAX': 10,     # Maximum token edits per sample\n",
    "    'P_MAX': 100,   # Maximum pixel edits per sample  \n",
    "    'Q_MAX': 100    # Maximum queries per sample\n",
    "}\n",
    "\n",
    "result = attack(test_image, test_pair['caption'], test_api, attack_budgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f7a10633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on validation set...\n",
      "Running on first 50 samples for quick testing...\n",
      "Starting evaluation...\n",
      "Evaluating on 50 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attacking:   0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Evaluate on subset first (faster for testing)\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning on first 50 samples for quick testing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 110\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_attack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattack_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbudgets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattack_budgets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Quick test on 50 samples\u001b[39;49;00m\n\u001b[1;32m    117\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEVALUATION RESULTS (50 samples):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[60], line 39\u001b[0m, in \u001b[0;36mevaluate_attack\u001b[0;34m(val_pairs, attack_function, alpha, beta, budgets, max_samples)\u001b[0m\n\u001b[1;32m     35\u001b[0m caption \u001b[38;5;241m=\u001b[39m pair[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Run attack\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mattack_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaption\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbudgets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Validate budget constraints\u001b[39;00m\n\u001b[1;32m     42\u001b[0m budget_valid \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     43\u001b[0m     result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_cost\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m budgets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT_MAX\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_cost\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m budgets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP_MAX\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m  \n\u001b[1;32m     45\u001b[0m     result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_cost\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m budgets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ_MAX\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     46\u001b[0m )\n",
      "Cell \u001b[0;32mIn[59], line 51\u001b[0m, in \u001b[0;36mattack\u001b[0;34m(image_np_uint8, caption_str, api, budgets)\u001b[0m\n\u001b[1;32m     28\u001b[0m original_score \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mscore(original_image, original_caption)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# For this baseline, we don't actually perform any attack\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Students should implement sophisticated attacks here!\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# - Query Optimization: Gradient-free optimization, genetic algorithms, hill climbing\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# - Multimodal: Combined text+image attacks for maximum effectiveness\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_image\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     52\u001b[0m image_grad \u001b[38;5;241m=\u001b[39m calc_image_grad(image_tensor, pair[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m], pair[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_match\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     53\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;66;03m# step size for adversarial perturbation\u001b[39;00m\n",
      "File \u001b[0;32m~/adverserial-cybersecurity/.venv/lib64/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/adverserial-cybersecurity/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/adverserial-cybersecurity/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/adverserial-cybersecurity/.venv/lib64/python3.12/site-packages/torchvision/transforms/transforms.py:972\u001b[0m, in \u001b[0;36mRandomResizedCrop.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    965\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be cropped and resized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Randomly cropped and resized image.\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 972\u001b[0m     i, j, h, w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mresized_crop(img, i, j, h, w, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation, antialias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mantialias)\n",
      "File \u001b[0;32m~/adverserial-cybersecurity/.venv/lib64/python3.12/site-packages/torchvision/transforms/transforms.py:933\u001b[0m, in \u001b[0;36mRandomResizedCrop.get_params\u001b[0;34m(img, scale, ratio)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_params\u001b[39m(img: Tensor, scale: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m], ratio: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    922\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get parameters for ``crop`` for a random sized crop.\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \n\u001b[1;32m    924\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;124;03m        sized crop.\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m     _, height, width \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m     area \u001b[38;5;241m=\u001b[39m height \u001b[38;5;241m*\u001b[39m width\n\u001b[1;32m    936\u001b[0m     log_ratio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(torch\u001b[38;5;241m.\u001b[39mtensor(ratio))\n",
      "File \u001b[0;32m~/adverserial-cybersecurity/.venv/lib64/python3.12/site-packages/torchvision/transforms/functional.py:80\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mget_dimensions(img)\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/adverserial-cybersecurity/.venv/lib64/python3.12/site-packages/torchvision/transforms/_functional_pil.py:34\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     32\u001b[0m     width, height \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39msize\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [channels, height, width]\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Unexpected type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "# Evaluation Framework\n",
    "def evaluate_attack(val_pairs: list, attack_function, alpha: float, beta: float, budgets: dict, max_samples: int = None):\n",
    "    \"\"\"\n",
    "    Evaluate attack function on validation pairs.\n",
    "    \n",
    "    Args:\n",
    "        val_pairs: List of validation pairs\n",
    "        attack_function: Attack function to evaluate\n",
    "        alpha, beta: Calibrated parameters\n",
    "        budgets: Attack budgets dictionary\n",
    "        max_samples: Limit number of samples (None = all)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting evaluation...\")\n",
    "    \n",
    "    # Limit samples if specified\n",
    "    eval_pairs = val_pairs[:max_samples] if max_samples else val_pairs\n",
    "    print(f\"Evaluating on {len(eval_pairs)} samples\")\n",
    "    \n",
    "    results = []\n",
    "    total_success = 0\n",
    "    total_token_cost = 0\n",
    "    total_pixel_cost = 0\n",
    "    total_query_cost = 0\n",
    "    \n",
    "    for i, pair in enumerate(tqdm(eval_pairs, desc=\"Attacking\")):\n",
    "        # Create fresh API instance for each sample\n",
    "        api = BlackBoxAPI(alpha, beta, q_max=budgets['Q_MAX'])\n",
    "        \n",
    "        # Load image\n",
    "        image = np.array(load_image_from_pair(pair))\n",
    "        caption = pair['caption']\n",
    "        \n",
    "        # try:\n",
    "        # Run attack\n",
    "        result = attack_function(image, caption, api, budgets)\n",
    "        \n",
    "        # Validate budget constraints\n",
    "        budget_valid = (\n",
    "            result['token_cost'] <= budgets['T_MAX'] and\n",
    "            result['pixel_cost'] <= budgets['P_MAX'] and  \n",
    "            result['query_cost'] <= budgets['Q_MAX']\n",
    "        )\n",
    "        \n",
    "        if not budget_valid:\n",
    "            print(f\"Sample {i}: Budget violation!\")\n",
    "            print(f\"   Tokens: {result['token_cost']}/{budgets['T_MAX']}\")\n",
    "            print(f\"   Pixels: {result['pixel_cost']}/{budgets['P_MAX']}\")  \n",
    "            print(f\"   Queries: {result['query_cost']}/{budgets['Q_MAX']}\")\n",
    "            result['success'] = False  # Invalid attacks count as failures\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        if result['success']:\n",
    "            total_success += 1\n",
    "        total_token_cost += result['token_cost']\n",
    "        total_pixel_cost += result['pixel_cost']\n",
    "        total_query_cost += result['query_cost']\n",
    "            \n",
    "        # except Exception as e:\n",
    "        #     print(f\"Sample {i}: Attack failed with error: {e}\")\n",
    "        #     # Add failed result\n",
    "        #     results.append({\n",
    "        #         'success': False,\n",
    "        #         'token_cost': budgets['T_MAX'],  # Penalize failures\n",
    "        #         'pixel_cost': budgets['P_MAX'], \n",
    "        #         'query_cost': budgets['Q_MAX'],\n",
    "        #         'error': str(e)\n",
    "        #     })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    n_samples = len(results)\n",
    "    asr = total_success / n_samples  # Attack Success Rate\n",
    "    anc = (10*total_token_cost + total_pixel_cost) / n_samples  # Average Number of Changes  \n",
    "    aq = total_query_cost / n_samples  # Average Queries\n",
    "    \n",
    "    # Final score: ASR - 0.5*ANC - 0.1*(AQ/Q_MAX)\n",
    "    score = asr - 0.5 * (anc / (10*budgets['T_MAX'] + budgets['P_MAX'])) - 0.1 * (aq / budgets['Q_MAX'])\n",
    "    \n",
    "    evaluation_result = {\n",
    "        'ASR': asr,\n",
    "        'ANC': anc, \n",
    "        'AQ': aq,\n",
    "        'Score': score,\n",
    "        'n_samples': n_samples,\n",
    "        'total_success': total_success,\n",
    "        'avg_token_cost': total_token_cost / n_samples,\n",
    "        'avg_pixel_cost': total_pixel_cost / n_samples,\n",
    "        'budgets': budgets,\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    return evaluation_result\n",
    "\n",
    "# Run Evaluation\n",
    "print(\"Running evaluation on validation set...\")\n",
    "\n",
    "# Define attack budgets\n",
    "attack_budgets = {\n",
    "    'T_MAX': 10,     # Maximum token edits per sample\n",
    "    'P_MAX': 100,   # Maximum pixel edits per sample  \n",
    "    'Q_MAX': 100    # Maximum queries per sample\n",
    "}\n",
    "\n",
    "# Evaluate on subset first (faster for testing)\n",
    "print(\"Running on first 50 samples for quick testing...\")\n",
    "eval_result = evaluate_attack(\n",
    "    val_pairs=val_pairs, \n",
    "    attack_function=attack,\n",
    "    alpha=alpha,\n",
    "    beta=beta, \n",
    "    budgets=attack_budgets,\n",
    "    max_samples=50  # Quick test on 50 samples\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nEVALUATION RESULTS (50 samples):\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Attack Success Rate (ASR): {eval_result['ASR']:.1%}\")\n",
    "print(f\"Average Number of Changes (ANC): {eval_result['ANC']:.2f}\")  \n",
    "print(f\"Average Queries (AQ): {eval_result['AQ']:.1f}\")\n",
    "print(f\"Final Score: {eval_result['Score']:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Budget Usage:\")\n",
    "print(f\"  Avg Token Cost: {eval_result['avg_token_cost']:.2f}/{attack_budgets['T_MAX']}\")\n",
    "print(f\"  Avg Pixel Cost: {eval_result['avg_pixel_cost']:.2f}/{attack_budgets['P_MAX']}\")  \n",
    "print(f\"  Avg Query Cost: {eval_result['AQ']:.1f}/{attack_budgets['Q_MAX']}\")\n",
    "print(f\"\\nNOTE: This is a trivial baseline (0% ASR expected)\")\n",
    "print(f\"Students should implement sophisticated attacks to improve ASR!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a906503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running FULL evaluation on all 1000 validation samples...\n",
      "This may take several minutes depending on your attack implementation.\n",
      "Starting evaluation...\n",
      "Evaluating on 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attacking: 100%|██████████| 1000/1000 [00:34<00:00, 28.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL EVALUATION RESULTS:\n",
      "============================================================\n",
      "Attack Success Rate (ASR): 0.1%\n",
      "Average Number of Changes (ANC): 0.00\n",
      "Average Queries (AQ): 2.0\n",
      "Final Score: -0.0010\n",
      "============================================================\n",
      "To run full evaluation on all 1000 samples:\n",
      "Uncomment: full_results = run_full_evaluation()\n",
      "\n",
      "Current status: Framework ready for student implementations!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Full Evaluation (Uncomment when ready to test your attack)\n",
    "\n",
    "def run_full_evaluation():\n",
    "    \"\"\"Run evaluation on all 1000 validation samples.\"\"\"\n",
    "    print(\"Running FULL evaluation on all 1000 validation samples...\")\n",
    "    print(\"This may take several minutes depending on your attack implementation.\")\n",
    "    \n",
    "    full_result = evaluate_attack(\n",
    "        val_pairs=val_pairs,\n",
    "        attack_function=attack, \n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        budgets=attack_budgets,\n",
    "        max_samples=None  # All samples\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFINAL EVALUATION RESULTS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Attack Success Rate (ASR): {full_result['ASR']:.1%}\")\n",
    "    print(f\"Average Number of Changes (ANC): {full_result['ANC']:.2f}\")\n",
    "    print(f\"Average Queries (AQ): {full_result['AQ']:.1f}\")\n",
    "    print(f\"Final Score: {full_result['Score']:.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return full_result\n",
    "\n",
    "# Uncomment the line below when ready to run full evaluation:\n",
    "full_results = run_full_evaluation()\n",
    "\n",
    "print(\"To run full evaluation on all 1000 samples:\")\n",
    "print(\"Uncomment: full_results = run_full_evaluation()\")\n",
    "print(\"\\nCurrent status: Framework ready for student implementations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41963ede",
   "metadata": {},
   "source": [
    "## Baseline scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0498a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_attack(image_np_uint8: np.ndarray, caption_str: str, api: BlackBoxAPI, budgets: dict) -> dict:\n",
    "\n",
    "    # TRIVIAL BASELINE - Students should replace this!\n",
    "    # This baseline just returns the original inputs without any attack\n",
    "    \n",
    "    original_image = image_np_uint8.copy()\n",
    "    original_caption = caption_str\n",
    "    \n",
    "    # Get original score to determine target (flip the decision)\n",
    "    original_score = api.score(original_image, original_caption)\n",
    "\n",
    "    final_image = original_image        # Comment out this line when implementing image attacks\n",
    "    final_caption = original_caption    # Comment out this line when implementing text attacks\n",
    "    \n",
    "    # Get final score  \n",
    "    final_score = api.score(final_image, final_caption)\n",
    "    \n",
    "    # Check if attack succeeded (decision flipped)\n",
    "    original_decision = original_score > 0.5\n",
    "    final_decision = final_score > 0.5\n",
    "    success = (original_decision != final_decision)\n",
    "    \n",
    "    # Calculate costs\n",
    "    token_cost = token_edit_cost(original_caption, final_caption)\n",
    "    pixel_cost = pixel_edit_cost(original_image, final_image)\n",
    "    query_cost = api.query_count\n",
    "    \n",
    "    return {\n",
    "        'success': success,\n",
    "        'image': final_image,\n",
    "        'caption': final_caption,\n",
    "        'token_cost': token_cost,\n",
    "        'pixel_cost': pixel_cost,  \n",
    "        'query_cost': query_cost,\n",
    "        'original_score': original_score,\n",
    "        'final_score': final_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d6f15b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on validation set...\n",
      "Running on first 50 samples for quick testing...\n",
      "Starting evaluation...\n",
      "Evaluating on 50 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attacking: 100%|██████████| 50/50 [00:01<00:00, 28.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EVALUATION RESULTS (50 samples):\n",
      "==================================================\n",
      "Attack Success Rate (ASR): 0.0%\n",
      "Average Number of Changes (ANC): 0.00\n",
      "Average Queries (AQ): 2.0\n",
      "Final Score: -0.0020\n",
      "==================================================\n",
      "Budget Usage:\n",
      "  Avg Token Cost: 0.00/10\n",
      "  Avg Pixel Cost: 0.00/100\n",
      "  Avg Query Cost: 2.0/100\n",
      "\n",
      "NOTE: This is a trivial baseline (0% ASR expected)\n",
      "Students should implement sophisticated attacks to improve ASR!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Run Evaluation\n",
    "print(\"Running evaluation on validation set...\")\n",
    "\n",
    "# Define attack budgets\n",
    "attack_budgets = {\n",
    "    'T_MAX': 10,     # Maximum token edits per sample\n",
    "    'P_MAX': 100,   # Maximum pixel edits per sample  \n",
    "    'Q_MAX': 100    # Maximum queries per sample\n",
    "}\n",
    "\n",
    "# Evaluate on subset first (faster for testing)\n",
    "print(\"Running on first 50 samples for quick testing...\")\n",
    "eval_result = evaluate_attack(\n",
    "    val_pairs=val_pairs, \n",
    "    attack_function=baseline_attack,\n",
    "    alpha=alpha,\n",
    "    beta=beta, \n",
    "    budgets=attack_budgets,\n",
    "    max_samples=50  # Quick test on 50 samples\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nEVALUATION RESULTS (50 samples):\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Attack Success Rate (ASR): {eval_result['ASR']:.1%}\")\n",
    "print(f\"Average Number of Changes (ANC): {eval_result['ANC']:.2f}\")  \n",
    "print(f\"Average Queries (AQ): {eval_result['AQ']:.1f}\")\n",
    "print(f\"Final Score: {eval_result['Score']:.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Budget Usage:\")\n",
    "print(f\"  Avg Token Cost: {eval_result['avg_token_cost']:.2f}/{attack_budgets['T_MAX']}\")\n",
    "print(f\"  Avg Pixel Cost: {eval_result['avg_pixel_cost']:.2f}/{attack_budgets['P_MAX']}\")  \n",
    "print(f\"  Avg Query Cost: {eval_result['AQ']:.1f}/{attack_budgets['Q_MAX']}\")\n",
    "print(f\"\\nNOTE: This is a trivial baseline (0% ASR expected)\")\n",
    "print(f\"Students should implement sophisticated attacks to improve ASR!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "307d8f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running FULL evaluation on all 1000 validation samples...\n",
      "This may take several minutes depending on your attack implementation.\n",
      "Starting evaluation...\n",
      "Evaluating on 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attacking: 100%|██████████| 1000/1000 [00:34<00:00, 28.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL EVALUATION RESULTS:\n",
      "============================================================\n",
      "Attack Success Rate (ASR): 0.6%\n",
      "Average Number of Changes (ANC): 0.00\n",
      "Average Queries (AQ): 2.0\n",
      "Final Score: 0.0040\n",
      "============================================================\n",
      "To run full evaluation on all 1000 samples:\n",
      "Uncomment: full_results = run_full_evaluation()\n",
      "\n",
      "Current status: Framework ready for student implementations!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Full Evaluation (Uncomment when ready to test your attack)\n",
    "\n",
    "def run_full_evaluation():\n",
    "    \"\"\"Run evaluation on all 1000 validation samples.\"\"\"\n",
    "    print(\"Running FULL evaluation on all 1000 validation samples...\")\n",
    "    print(\"This may take several minutes depending on your attack implementation.\")\n",
    "    \n",
    "    full_result = evaluate_attack(\n",
    "        val_pairs=val_pairs,\n",
    "        attack_function=baseline_attack, \n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        budgets=attack_budgets,\n",
    "        max_samples=None  # All samples\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFINAL EVALUATION RESULTS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Attack Success Rate (ASR): {full_result['ASR']:.1%}\")\n",
    "    print(f\"Average Number of Changes (ANC): {full_result['ANC']:.2f}\")\n",
    "    print(f\"Average Queries (AQ): {full_result['AQ']:.1f}\")\n",
    "    print(f\"Final Score: {full_result['Score']:.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return full_result\n",
    "\n",
    "# Uncomment the line below when ready to run full evaluation:\n",
    "full_results = run_full_evaluation()\n",
    "\n",
    "print(\"To run full evaluation on all 1000 samples:\")\n",
    "print(\"Uncomment: full_results = run_full_evaluation()\")\n",
    "print(\"\\nCurrent status: Framework ready for student implementations!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
